{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objbrowser import browse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyphenate corpus with stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/rnn_generator')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path('data/rnn_generator'); dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1280\n",
      "drwxr-xr-x  5 wcz  staff   160B Apr 30 13:56 \u001b[34m.\u001b[m\u001b[m/\n",
      "drwxr-xr-x  4 wcz  staff   128B Apr 30 13:27 \u001b[34m..\u001b[m\u001b[m/\n",
      "-rw-r--r--  1 wcz  staff   362K Apr 30 13:58 pan_tadeusz.syl1.txt\n",
      "-rw-rw-rw-@ 1 wcz  staff   218K Apr 23 19:58 pan_tadeusz.txt\n",
      "drwxr-xr-x  2 wcz  staff    64B Apr 30 13:27 \u001b[34mtmp\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls -lah $dataset_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hyphenated corpus\n",
    "# cleanup and tokenize later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
    "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyphenate corpus with stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer 2.0.3 2018-04-19 (macOS i386)\n",
      "For Korrida database, spellchecker and hyphenator copyright (C) 1993-2018 Wojciech Czarnowski\n",
      "For Stemmer copyright (C) 2018 Krzysztof Wolk and Wojciech Czarnowski\n",
      "Wojciech Czarnowski: wojtek.czarnowski@gmail.com, +48(608)202-272\n",
      "Krzysztof Wolk: krz.wolk@gmail.com, +48(606)918-623\n",
      "\n",
      "Usage: stemmer options\n",
      "\n",
      "Options:\n",
      "-v, --verbose\n",
      "--version\n",
      "-s number, --stem=number\n",
      "-d file or --dict=<file.dic>\n",
      "-i file or --file_in=<file>\n",
      "-o file or --file_out=<file>\n",
      "-l file or --log=<file>\n",
      "-e file or --exclude=<file>\n",
      "--stem_delim=\"string\"\n",
      "--code_delim=\"string\"\n",
      "--divide_after=number\n",
      "\n",
      "Stemming options:\n",
      "  1 stem \"nie\" prefix\n",
      "  2 stem extra prefixes\n",
      "  4 stem prefixes in root\n",
      "  8 stem in suffix\n",
      "\n",
      "POS options:\n",
      " 16 output POS and inflection group\n",
      " 32 output additional POS info\n",
      "\n",
      "Output Korrida codes:\n",
      " 64 group code\n",
      "128 base suffix codes\n",
      "256 suffix code\n",
      "\n",
      "Division options:\n",
      " 512 divide words\n",
      "1024 divide with dictionary\n",
      "2048 divide algorithmically\n",
      "4096 divide unknown words\n",
      "\n",
      "Using 512 division option turns off stemming.\n",
      "\n",
      "Defaults:\n",
      "- stem delimiter: \"++ --\"\n",
      "- code delimiter: \"@@\"\n",
      "- options: 1+2\n",
      "- divide after 1 character\n",
      "\n",
      "Sample sets of options:\n",
      "3=1+2          (stem more words)\n",
      "15=1+2+3+4+8   (stem more words and in roots and suffixes)\n",
      "320=64+256     (output old Korrida codes)\n",
      "195=1+2+64+128 (stem more words, output new Korrida codes)\n",
      "147=1+2+16+128 (stem more words, output POS and base suffix codes)\n",
      "7683=1+2+512+1024+2048+4096 (divide (more) words with dictionary, then algorithmically and also unknown words)\n"
     ]
    }
   ],
   "source": [
    "!bin/stemmer.macos -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer 2.0.3 2018-04-19 (macOS i386)\n",
      "For Korrida database, spellchecker and hyphenator copyright (C) 1993-2018 Wojciech Czarnowski\n",
      "For Stemmer copyright (C) 2018 Krzysztof Wolk and Wojciech Czarnowski\n",
      "Wojciech Czarnowski: wojtek.czarnowski@gmail.com, +48(608)202-272\n",
      "Krzysztof Wolk: krz.wolk@gmail.com, +48(606)918-623\n",
      "\n",
      "Dictionary: \"bin/stemmer2.dic\"\n",
      "Input file: \"data/rnn_generator/pan_tadeusz.txt\"\n",
      "Output file: \"data/rnn_generator/pan_tadeusz.syl1.txt\"\n",
      "Stem number: \"7683\"\n",
      "\n",
      "Stemming options:\n",
      "  StemNiePrefix     : Yes\n",
      "  StemExtraPrefixes : Yes\n",
      "  StemPrefixesInRoot: No\n",
      "\n",
      "Syllable division options:\n",
      "  DivideWords          : Yes\n",
      "  DivideWithDictionary : Yes\n",
      "  DivideAlgorithmically: Yes\n",
      "  DivideUknkownWords   : Yes\n",
      "  divideAfterChar      : 1\n",
      "\n",
      "Stemming formatting options:\n",
      "  StemInSuffix       : No\n",
      "  ShowPOSInfo        : No\n",
      "  ShowExtraPOSInfo   : No\n",
      "\n",
      "  ShowGroupCode      : No\n",
      "  ShowBaseSuffixCodes: No\n",
      "  ShowSuffixCode     : No\n",
      "\n",
      "  stemDelimiterStr   : \"++ --\"\n",
      "  codeDelimiterStr   : \"@@\"\n",
      "\n",
      "StemFile(fileInPath: \"data/rnn_generator/pan_tadeusz.txt\", fileOutPath: \"data/rnn_generator/pan_tadeusz.syl1.txt\")\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!bin/stemmer.macos -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_char -o $fn_corpus_syl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load char corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffKSIĘGA PIÉRWSZA.\\n\\n\\n\\n\\nGOSPODARSTWO.\\n\\n\\nTREŚĆ.\\n\\n    Powrot panicza -- Spotkanie się piérwsze w pokoiku, drugie u\\n    stołu -- Ważna Sędziego nauka o grzeczności -- Podkomorzego uwagi\\n    polityczne nad modami -- Początek sporu o Kusego i Sokoła -- Żale\\n    Wojskiego -- Ostatni Woźny Trybunału -- Rzut oka na ówczesny stan\\n    polityczny Litwy i Europy.\\n\\n\\n  Litwo! Ojczyzno moja! ty jesteś jak zdrowie;\\nIle cię trzeba cenić, ten tylko się dowie\\nKto cię stracił. Dziś piękność twą w całéj ozdobie\\nWidzę '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_char = fn_corpus_char.open('r').read(); corpus_char[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load hyphenated corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KSIĘ++ --GA PIÉ++ --RWSZA.\\n\\n\\n\\n\\nGO++ --S++ --PO++ --DAR++ --STWO.\\n\\n\\nTREŚĆ.\\n\\n    Po++ --wrot pa++ --ni++ --cza -- Spot++ --ka++ --nie się pié++ --rwsze w po++ --koi++ --ku, dru++ --gie u\\n    sto++ --łu -- Waż++ --na Sę++ --dzie++ --go na++ --u++ --ka o grze++ --cz++ --no++ --ści -- Pod++ --ko++ --mo++ --rze++ --go u++ --wa++ --gi\\n    po++ --li++ --ty++ --cz++ --ne nad mo++ --da++ --mi -- Po++ --czą++ --tek spo++ --ru o Ku++ --se++ --go i So++ --ko++ --ła -- Ża++ --le\\n    Woj++ --skie++ --go -- O++'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hyphenated corpus\n",
    "corpus_syl = fn_corpus_syl.open('r').read(); corpus_syl[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into tokens\n",
    "# create set of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KSIĘ++ --GA PIÉ++ --RWSZA.\\n\\n\\n\\n\\nGO++ --S++ --PO++ --DAR++ --STWO.\\n\\n\\nTREŚĆ.\\n\\n    Po++ --wrot pa++ --ni++ --cza -- Spot++ --ka++ --nie się pié++ --rwsze w po++ --koi++ --ku, dru++ --gie u\\n    sto++ --łu -- Waż++ --na Sę++ --dzie++ --go na++ --u++ --ka o grze++ --cz++ --no++ --ści -- Pod++ --ko++ --mo++ --rze++ --go u++ --wa++ --gi\\n    po++ --li++ --ty++ --cz++ --ne nad mo++ --da++ --mi -- Po++ --czą++ --tek spo++ --ru o Ku++ --se++ --go i So++ --ko++ --ła -- Ża++ --le\\n    Woj++ --skie++ --go -- O++'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_syl[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from fastai/text.py\n",
    "import re, string\n",
    "# remove +,- chars from punctuation set to keep sylables e.g.'--PO++' intact\n",
    "punctuation=re.sub('[\\+-]', '', string.punctuation)\n",
    "re_tok = re.compile(f'([{punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffKSIĘGA', 'PIÉRWSZA', '.', 'GOSPODARSTWO', '.', 'TREŚĆ', '.', 'Powrot', 'panicza', '--', 'Spotkanie', 'się', 'piérwsze', 'w', 'pokoiku', ',', 'drugie', 'u', 'stołu', '--', 'Ważna', 'Sędziego', 'nauka', 'o', 'grzeczności', '--', 'Podkomorzego', 'uwagi', 'polityczne', 'nad']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(corpus_char[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68475, ['KSIĘ++', '--GA', 'PIÉ++', '--RWSZA', '.', 'GO++', '--S++', '--PO++'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_syl_tok = tokenize(corpus_syl); len(corpus_syl_tok), corpus_syl_tok[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5776 ['!', '\"', '%', \"'\", '(', ')', ',', '--', '--CIA', '--DAR++', '--GA', '--GI', '--KA', '--LO++', '--MA++', '--MEK', '--MIZ++', '--NIA', '--PO++', '--RWSZA', '--S++', '--STWO', '--TA', '--TY++', '--WY', '--a++', '--aczéj', '--ał', '--b++', '--ba', '--ba++', '--bach', '--baj++', '--bak', '--ban', '--ban++', '--bar++', '--barz', '--baw', '--baw++', '--bał', '--baż', '--bcem', '--be++', '--bek', '--bel', '--bel++', '--belg', '--bem', '--ber++', '--bez++', '--beł', '--bi', '--bi++', '--bia', '--bia++', '--biad', '--biar++', '--biać', '--biał', '--bic', '--bie', '--bie++', '--biec', '--biedz', '--bieg++', '--biegł', '--biem', '--bier++', '--biet', '--biet++', '--bieć', '--bień++', '--bij++', '--bik', '--bim', '--bin', '--bin++', '--bio', '--bio++', '--bior', '--bios', '--bis++', '--bisz', '--bit++', '--biu', '--biw++', '--bié++', '--biór', '--biów', '--bią', '--bią++', '--biąc', '--bić', '--bię', '--bił', '--bił++', '--bla++', '--blad++', '--bled++', '--bli++', '--blis++', '--bliż++', '--bliżéj', '--blu', '--bnie', '--bnych', '--bną', '--bo', '--bo++', '--bo-bra++', '--boc++', '--boj++', '--bok', '--bom', '--bor', '--bor++', '--boszcz', '--bot++', '--boż++', '--br++', '--bra++', '--braw++', '--brać', '--brał', '--bre', '--bre++', '--brno++', '--bro++', '--bron++', '--bru++', '--bry', '--bry++', '--brych', '--brym', '--brzmia++', '--brzy++', '--brzyń++', '--bu', '--bu++', '--buch', '--bud++', '--bum', '--bun', '--bur++', '--by', '--by++', '--bym', '--byt++', '--bytéj']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(corpus_syl_tok)))\n",
    "n_tokens = len(all_tokens); print(n_tokens, all_tokens[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert a syllable token list to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--ka++', '--nie', 'się', 'pié++', '--rwsze', 'w', 'po++', '--koi++', '--ku', ',']\n",
      "Variable containing:\n",
      "  948\n",
      " 1379\n",
      " 4856\n",
      " 4619\n",
      " 1641\n",
      " 5218\n",
      " 4643\n",
      " 1016\n",
      " 1062\n",
      "    6\n",
      "[torch.LongTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_characters = all_tokens\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(to_gpu(tensor))\n",
    "\n",
    "a_token_list = corpus_syl_tok[20:30]; print(a_token_list)\n",
    "print(char_tensor(a_token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random chunk of syllabized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ze łza/mi wpad/ła w me ob/ję/cia . Gdym przy/był do Pa/ler/mo , wie/dzia/no z ga/ze/ty , Pal/ca/mi wska/zy/wa/ły mię wszy/s/t/kie ko/bie/ty . Na/wet wy/dru/ko/wa/no o całém zda/rze/niu Ro/mans , gdzie wy/mie/nio/ny je/s/tem po i/mie/niu . Ro/mans ma ty/tuł : _ Hra/bia , czy/li ta/jem/ni/ce Za/mku Bir/ban/te -- rok/ka _ . Czy są tu\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 100\n",
    "\n",
    "\n",
    "file = corpus_syl_tok\n",
    "file_len = len(file)\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(' '.join(random_chunk()).replace('++ --', '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, t = random_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1367\n",
      "    6\n",
      " 3271\n",
      "  171\n",
      " 4856\n",
      " 5218\n",
      " 4653\n",
      " 2394\n",
      "    6\n",
      " 5218\n",
      "[torch.LongTensor of size 10]\n",
      "\n",
      "Variable containing:\n",
      "    6\n",
      " 3271\n",
      "  171\n",
      " 4856\n",
      " 5218\n",
      " 4653\n",
      " 2394\n",
      "    6\n",
      " 5218\n",
      " 4343\n",
      "[torch.LongTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a[:10])\n",
    "print(t[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
