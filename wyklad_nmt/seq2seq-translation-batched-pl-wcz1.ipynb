{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pi0-vQzjM-a"
   },
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Tłumaczenie z użyciem sieci Sequence to Sequence z koncentracją\n",
    "\n",
    "\n",
    "W tym projekcie nauczymy sieci neuronowej, aby przetłumaczyć z angielskiego na polski, przykłady jednak poniższe pokazują zasadę działania na przykładzie tłumaczenia z francuskiego na angielski.\n",
    "\n",
    "```\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "\n",
    "\n",
    "... w różnym stopniu sukcesu.\n",
    "\n",
    "Jest to możliwe dzięki prostej, ale potężnej idei sieci [sequence to sequence](http://arxiv.org/abs/1409.3215), w której dwie rekurencyjne sieci neuronowe współpracują ze sobą, aby przekształcić jedną sekwencję w drugą. Sieć koderów kondensuje sekwencję wejściową do pojedynczego wektora, a sieć dekodera rozwija ten wektor w nową sekwencję.\n",
    "\n",
    "Aby ulepszyć ten model, użyjemy [mechanizmu koncentracji](https://arxiv.org/abs/1409.0473), który pozwoli dekoderowi nauczyć się skupiać na określonym zakresie sekwencji wejściowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bO8jBTPCjM-b"
   },
   "source": [
    "# Model Sequence to Sequence\n",
    "\n",
    "\n",
    "Sieć [Sequence to Sequence](http://arxiv.org/abs/1409.3215) lub sieć seq2seq, lub sieć [Encoder Decoder](https://arxiv.org/pdf/1406.1078v3.pdf), jest to model składający się z dwóch oddzielnych RNN'ów zwanych **koderem** i **dekoderem**. W każdym kroku koder odczytuje sekwencję wejściową po jednym elemencie a oddaje wektor. Końcowy wynik kodera jest utrzymywany jako wektor **kontekstu**. Dekoder wykorzystuje ten wektor kontekstu, aby wytworzyć sekwencję wyjść krok po kroku.\n",
    "\n",
    "![](https://i.imgur.com/tVtHhNp.png)\n",
    "\n",
    "Podczas korzystania z pojedynczego RNN istnieje relacja jeden do jednego pomiędzy wejściami i wyjściami. Szybko napotkalibyśmy problemy z różnymi kolejnościami sekwencji i długościami, które są częste podczas tłumaczenia. Rozważ proste zdanie \"Je ne suis pas le chat noir\" & rarr; \"I am not the black cat\" (\"Nie jestem czarnym kotem\"). Wiele słów ma całkiem jednoznaczne tłumaczenie, na przykład \"chat\" &rarr; \"cat\". Jednak różne gramatyki powodują, że słowa występują w różnych kolejnościach, np. \"chat noir\" &rarr; \"black cat\". Istnieje również \"ne ... pas\" &rarr; \"not\" konstrukcja, która sprawia, że dwa zdania mają różne długości.\n",
    "\n",
    "Model seq2seq, kodując wiele wejść do jednego wektora i dekodując z jednego wektora na wiele wyjść,  uwalnia nas od ograniczeń kolejności i długości sekwencji. Zakodowana sekwencja jest reprezentowana przez pojedynczy wektor, pojedynczy punkt w N-wymiarowej przestrzeni sekwencji. W idealnym przypadku punkt ten można uznać za \"znaczenie\" sekwencji.\n",
    "\n",
    "Ten pomysł może zostać przedłużony poza sekwencje. Zadania opisywania obrazów pobierają [obraz jako dane wejściowe i produkują opis](https://arxiv.org/abs/1411.4555) obrazu (img2seq). Niektóre zadania generowania obrazów pobierają [opis jako dane wyjściowe i generują obraz](https://arxiv.org/abs/1511.02793) (seq2img). Takie modele można ogólnie nazwać sieciami \"koder dekoder\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgcvjusvjM-c"
   },
   "source": [
    "## Mechanizm koncentracji (Attention)\n",
    "\n",
    "Wektor o stałej długości jest odpowiedzialny za zakodowanie całego \"znaczenia\" sekwencji wejściowej, bez względu na jej długość. Przy całej rożnorodności językowej jest to bardzo trudny problem. Wyobraź sobie dwa prawie identyczne zdania, dwadzieścia słów, z jednym tylko słowem innym. Zarówno kodery, jak i dekodery muszą być wystarczająco zniuansowane, aby reprezentować tę zmianę jako nieznacznie inny punkt w przestrzeni.\n",
    "\n",
    "**Mechanizm koncentracji** [wprowadzony przez Bahdanau i innych](https://arxiv.org/abs/1409.0473) rozwiązuje ten problem, dając dekoderowi możliwość \"koncentrowania się\" na fragmentach danych wejściowych, zamiast polegania na na pojedynczym wektorze. Dla każdego kroku dekoder może wybrać inną część sekwencji wejściowej do rozważenia.\n",
    "\n",
    "![](https://i.imgur.com/5y6SCvU.png)\n",
    "\n",
    "Koncentracja jest obliczana przy użyciu bieżącego stanu ukrytego i każdego wyjścia kodera, aby utworzyć nowy wektor, który ma taki sam rozmiar jak sekwencja wejściowa nazwany *wagami koncentracji*. Te wagi są mnożone przez wyjścia koderów w celu utworzenia nowego wektora *kontekstu*. Wektor kontekstu i stan ukryty jest następnie wykorzystywany do predykcji kolejnego elementu wyjścia.\n",
    "\n",
    "![](https://i.imgur.com/K1qMPxs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzYe2wPrjM-c"
   },
   "source": [
    "# Wymagania\n",
    "\n",
    "Będziesz potrzebował [PyTorch](http://pytorch.org/), aby zbudować i wyszkolić modele, i [matplotlib](https://matplotlib.org/), aby wykreślić postęp treningu, a później zwizualizować wyniki koncentracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HapeGWyXjM-e"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qk60_vl_jM-h"
   },
   "source": [
    "Tutaj również zdefiniujemy stałą, aby zdecydować, czy użyć GPU (w szczególności z CUDA), czy procesora. **Jeśli nie masz GPU, ustaw tę wartość na `False`**. Później, kiedy stworzymy tensory, zmienna ta będzie używana do decydowania o tym, czy zachowamy je na procesorze, czy przeniesiemy na GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TXlXt0dGjM-h"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCNzsA7ujM-k"
   },
   "source": [
    "# Ładowanie plików danych\n",
    "\n",
    "Dane do tego projektu to zbiór wielu tysięcy par tłumaczeń z angielskiego na polski.\n",
    "\n",
    "[To pytanie na temat Open Data Stack Exchange](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages) skierowało mnie na stronę otwartego tłumaczenia http://tatoeba.org/ który ma pliki do pobrania dostępne na stronie http://tatoeba.org/eng/downloads - a jeszcze lepiej, ktoś wykonał dodatkową pracę polegającą na podzieleniu par językowych na poszczególne pliki tekstowe tutaj: http://www.manythings.org/anki/\n",
    "\n",
    "Angielsko-polski plik pobierz używając naszego notebooka konfiguracyjnego. Plik składa się z oddzielonych tabulatorami par tłumaczeń:\n",
    "\n",
    "```\n",
    "I am cold.    Je suis froid.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJoHRQsMjM-k"
   },
   "source": [
    "Podobnie jak w przypadku kodowania znakowego, używanego w tutorialach \"char-rnn-\\*\", będziemy reprezentować każde słowo w języku jako wektor *1 z n* (ang. one-hot), czyli gigantyczny wektor zer, z wyjątkiem jednej jedynki (na indeksie słowa). W porównaniu z dziesiątkami znaków, które mogą istnieć w języku, istnieje o wiele więcej słów, więc i wektor kodowania jest znacznie większy. Będziemy jednak trochę oszukiwać i trymować dane, aby używać tylko kilku tysięcy słów na język."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVdtUNzxjM-l"
   },
   "source": [
    "### Indeksowanie słów\n",
    "\n",
    "Będziemy potrzebować unikalnego indeksu dla każdego słowa, który później posłuży jako dane wejściowe i wyjściowe. Aby śledzić to wszystko, użyjemy klasy pomocniczej o nazwie `Lang`, która ma słowniki słowo &rarr; indeks (`word2index`) i indeks &rarr; słowo (`index2word`), a także licznik każdego słowa `word2count`, którego można użyć do późniejszego zastąpienia rzadko występujących słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "38CWcoJijM-m"
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrjDYa90jM-o"
   },
   "source": [
    "### Czytanie i dekodowanie plików\n",
    "\n",
    "Wszystkie pliki są w Unicode, aby uprościć, zrobimy wszystko małymi literami i wycinamy większość znaków interpunkcyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1525693598389,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "y-uANRyqjM-p",
    "outputId": "05429291-5467-4870-d0c7-e96cd339ca11"
   },
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "#     s = unicode_to_ascii(s.lower().strip())\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "  \n",
    "normalize_string('ala ma kota ąćęłńóśźż ĄĆĘŁŃÓŚŹŻ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3_B-7AgjM-r"
   },
   "source": [
    "Aby odczytać plik danych, podzielimy go na linie, a następnie podzielimy linie na pary. Wszystkie pliki są \"angielski  &rarr; inny język\", więc jeśli chcemy przetłumaczyć z innego języka &rarr; angielski dodana została flaga `reverse`, aby odwrócić pary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Be5PbEG6jM-r"
   },
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    filename = 'data/seq2seq/%s-%s.txt' % (lang1, lang2)\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gbR0LICajM-u"
   },
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25\n",
    "# MIN_LENGTH = 1\n",
    "# MAX_LENGTH = 100\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) >= MIN_LENGTH and len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) >= MIN_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3d4PanL1jM-x"
   },
   "source": [
    "Pełny proces przygotowywania danych to:\n",
    "\n",
    "* Odczytaj plik tekstowy i podziel na linie, \n",
    "* Podziel linie na pary i znormalizuj, \n",
    "* Przefiltruj do par o maksymalnej długości\n",
    "* Stwórz listy słów ze zdań w parach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1599,
     "status": "ok",
     "timestamp": 1525693605452,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "gMHfcrOhjM-x",
    "outputId": "980905ef-2466-47b2-8e99-6c9b655ab029"
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %d sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "    \n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'pol', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BjKDGSYHjM-2"
   },
   "source": [
    "### Filtrowanie słowników\n",
    "\n",
    "Aby coś wytrenować w krócej niż godzinę, strymujmy nieco zestaw danych. Najpierw użyjemy funkcję `trim` w każdym języku (zdefiniowaną wcześniej), aby zachować tylko słowa, które powtarzają się przynajmniej określoną liczbę razy w zestawie danych (co zmniejszy trudność nauczenia się poprawnego tłumaczenia słów, które nie występują często)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 895,
     "status": "ok",
     "timestamp": 1525693639131,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "L8uPxAvUjM-2",
    "outputId": "5527d45b-61e4-41a7-c09d-8c8cca6b05b4"
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 3  # 5\n",
    "# MIN_COUNT = 2\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_batj5qjjM-5"
   },
   "source": [
    "### Filtrowanie par\n",
    "\n",
    "Teraz wrócimy do zestawu wszystkich par zdań i usuniemy te z nieznanymi słowami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1525693641763,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "3Ht7UIxjjM-6",
    "outputId": "50bf1f93-c73b-4e75-ee52-49d1c3374431"
   },
   "outputs": [],
   "source": [
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    \n",
    "    for word in input_sentence.split(' '):\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "\n",
    "    for word in output_sentence.split(' '):\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    # Remove if pair doesn't match input and output conditions\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBXR6h89jM-9"
   },
   "source": [
    "## Zamiana danych treningowych na Tensory\n",
    "\n",
    "Aby trenować, musimy przekształcić zdania w coś, co sieć neuronowa może zrozumieć, co oczywiście oznacza liczby. Każde zdanie zostanie podzielone na słowa i przekształcone w Tensor, gdzie każde słowo zostanie zastąpione indeksem (z wcześniej przygotowanych indeksów językowych). Podczas tworzenia tych tensorów dodamy również token EOS, aby zasygnalizować, że zdanie jest skończone.\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wtOXR7IkjM--"
   },
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence, plus EOS\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4cwKAbhjM_D"
   },
   "source": [
    "Możemy lepiej wykorzystać GPU, trenując wiele sekwencji naraz, ale czyniąc to, pojawia się pytanie, jak radzić sobie z sekwencjami o różnych długościach. Prostym rozwiązaniem jest \"wypełnienie\" krótszych zdań z pewnym symbolem wypełnienia (w tym przypadku `0`) i ignorowanie tych wypełnionych miejsc podczas obliczania straty.\n",
    "\n",
    "![](https://i.imgur.com/gGlkEEF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "X_n5XY_wjM_D"
   },
   "outputs": [],
   "source": [
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjbE-NkajM_F"
   },
   "source": [
    "Aby utworzyć zmienną dla pełnej partii danych wejściowych (i wyjściowych), otrzymujemy losową próbkę sekwencji i przypisujemy je do długości najdłuższej sekwencji. Będziemy śledzić długości każdej partii, aby później ją usunąć.\n",
    "\n",
    "Inicjowanie `LongTensor` za pomocą macierzy (partii) macierzy (sekwencji) daje nam tensor `(batch_size x max_len)` - wybór pierwszego wymiaru daje pojedynczą partię, która jest pełną sekwencją. Podczas szkolenia modelu będziemy potrzebować jednorazowego kroku naraz, więc przeniesiemy go do `(max_len x batch_size)`. Teraz wybieranie wzdłuż pierwszego wymiaru powoduje powrót do pojedynczego kroku w różnych partiach.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/nBxTG3v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QV-eXwgPjM_G"
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(output_lang, pair[1]))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mFgcJcnjM_I"
   },
   "source": [
    "Możemy przetestować to, aby zobaczyć, że zwróci on tensor `(max_len x batch_size)` dla zdań wejściowych i docelowych, wraz z odpowiadającą im listą długości partii dla każdego (które wykorzystamy później do maskowania)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5135,
     "status": "ok",
     "timestamp": 1525693658536,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "SRQMhAnnjM_J",
    "outputId": "66c6f672-2ea0-45c4-a181-cd2ecd56403d"
   },
   "outputs": [],
   "source": [
    "random_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKx3SlwgjM_K"
   },
   "source": [
    "# Budowanie modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFIBysF3jM_L"
   },
   "source": [
    "## Koder\n",
    "\n",
    "<img src=\"https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/images/encoder-network.png?raw=true\" style=\"float: right\" />\n",
    "\n",
    "Koder pobiera serię sekwencji słów, `LongTensor` o rozmiarze `(max_len x batch_size)` i wyprowadza kodowanie dla każdego słowa, `FloatTensor` o rozmiarze `(max_len x batch_size x hidden_size)`.\n",
    "\n",
    "Dane wejściowe są podawane przez [warstwę osadzającą `nn.Embedding`](http://pytorch.org/docs/nn.html#embedding) w celu utworzenia osadzenia dla każdego słowa, z rozmiarem `seq_len x hidden_size` (jeśli to była partia słów). Zostało to zmienione na `seq_len x 1 x hidden_size`, aby pasowało do oczekiwanego wejścia [warstwy GRU` nn.GRU`](http://pytorch.org/docs/nn.html#gru). GRU zwróci zarówno sekwencję wyjściową o rozmiarze `seq_len x hidden_size`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CWtfwbnwjM_M"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLWVsv1VjM_O"
   },
   "source": [
    "## Dekoder koncentracji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHtNHu50jM_O"
   },
   "source": [
    "### Interpretacja modelu Bahdanau\n",
    "\n",
    "Model koncentracji w [Neurowe tłumaczenie maszynowe przez wspólne uczenie się wyrównywania i tłumaczenia](https://arxiv.org/abs/1409.0473) jest opisany jako następująca seria równań:\n",
    "\n",
    "Każde wyjście dekodera jest uwarunkowane poprzednimi wyjściami i pewnym $\\mathbf x$, gdzie $\\mathbf x $składa się z bieżącego stanu ukrytego (który bierze pod uwagę poprzednie wyjścia) i \"kontekstu\" koncentracji, który jest obliczany poniżej. Funkcja $g$ jest w pełni połączoną warstwą z nieliniową aktywacją, która jako dane wejściowe przyjmuje połączone wartości $y_{i-1}$, $s_i$ i $c_i$.\n",
    "\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "Bieżący stan ukryty $s_i$ jest obliczany przez RNN $f$ z ostatnim ukrytym stanem $s_{i-1}$, ostatnią wartością wyjściową dekodera $y_{i-1}$  i wektorem kontekstu $c_i$.\n",
    "W kodzie RNN będzie warstwą `nn.GRU`, ukryty stan $s_i$ będzie nazywany `hidden`, wyjście $y_i$ zwane `output` oraz kontekst $c_i$ zwany `context`.\n",
    "\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "Wektor kontekstu $c_i$ jest sumą ważoną wszystkich wyników kodera, gdzie każda waga $a_{ij}$ jest ilością \"koncentracji\" przekazanej na odpowiednie wyjście kodera $h_j$.\n",
    "\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "... gdzie każda waga $a_{ij}$ to znormalizowana (ponad wszystkie kroki) koncentracja \"energia\" $e_{ij}$ ...\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "... gdzie każda energia koncentracji jest obliczana za pomocą funkcji $a$ (takiej jak inna warstwa liniowa) przy użyciu ostatniego ukrytego stanu $s_{i-1}$ i tego konkretnego wyjścia kodera $h_j$:\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVGfY39sjM_O"
   },
   "source": [
    "### Interpretacja modelu Luonga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "079wjeknjM_P"
   },
   "source": [
    "[Skuteczne podejście do neuronowego tłumaczenia maszynowego z naciskiem na koncentrację](https://arxiv.org/abs/1508.04025) autorstwa Luonga i innych. opisuje kilka modeli koncentracji, które oferują ulepszenia i uproszczenia. Opisują kilka modeli \"globalnej koncentracji\", a rozróżnianych przez sposób obliczania wyników koncentracji.\n",
    "\n",
    "Ogólna forma obliczania koncentracji opiera się na ukrytym stanie po stronie docelowej (dekodera) i odpowiednim stanie bocznym źródła (kodera), znormalizowanym dla wszystkich stanów, aby uzyskać wartości zsumowane do 1:\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "Konkretną funkcją \"oceny\" (score), która porównuje dwa stany, jest albo *dot*, iloczyn skalarny między stanami; *general*, iloczyn skalarny między ukrytym stanem dekodera a liniową transformacją stanu kodera; lub *concat*, iloczyn skalarny między nowym parametrem $v_a$ a liniową transformacją stanów połączonych ze sobą.\n",
    "\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & dot \\\\\n",
    "h_t ^\\top \\textbf{W}_a \\bar h_s & general \\\\\n",
    "v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ] & concat\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Modułowa definicja tych funkcji ewaluacji daje nam możliwość zbudowania specjalnego modułu koncentracji, który może przełączać się między różnymi metodami oceny. Wejście do tego modułu to zawsze stan ukryt (dekodera RNN) i zestaw wyjść kodera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tvHPEhbjM_Q"
   },
   "source": [
    "### Implementacja modułu koncentracji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tvQQ9_O6jM_Q"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies, dim=-1).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy =torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.view(-1), energy.view(-1))\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdQV7jCLjM_R"
   },
   "source": [
    "### Implementacja modelu Bahdanau\n",
    "\n",
    "Podsumowując, nasz dekoder powinien składać się z czterech głównych części - warstwy osadzającej, zamieniającej słowo wejściowe w wektor; warstwa do obliczenia energii koncentracji na wyjście kodera; warstwa RNN; i warstwę wyjściową.\n",
    "\n",
    "Wejścia dekodera są ostatnim ukrytym stanem RNN $s_{i-1}$, ostatnie wyjście $y_{i-1}$ oraz wszystkie wyjścia kodera $h_*$.\n",
    "\n",
    "* warstwa osadzająca z wejściami $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* warstwa koncentracji $a$ z wejściami $(s_{i-1}, h_j)$ i wyjściami $e_{ij}$, znormalizowanymi w celu utworzenia $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* wektor kontekstowy $c_i$ jako średnia ważona wyjść kodera\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* warstwa (warstwy) RNN $f$ z wejściami $(s_{i-1}, y_{i-1}, c_i)$ i wewnętrznym stanem ukrytym, wyprowadzanie $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* warstwa wyjściowa $g$ z wejściami $(y_{i-1}, s_i, c_i)$, wyprowadzanie $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Cq2AurVKjM_S"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)), dim=-1)\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qq9r0senjM_U"
   },
   "source": [
    "Teraz możemy zbudować dekoder, który podłącza ten moduł Attn za RNN, aby obliczyć wagi koncentracji, i zastosować te wagi do wyjść kodera, aby uzyskać wektor kontekstu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8WQ5ZymQjM_U"
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GlQFiBXjM_W"
   },
   "source": [
    "## Testowanie modeli\n",
    "\n",
    "Aby upewnić się, że moduły kodera i dekodera działają (i współpracują ze sobą), wykonamy pełny test przy pomocy małej partii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1525693677221,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "1Jl3G8A3jM_W",
    "outputId": "b429c793-4c78-4999-88c9-c74d19a0f348"
   },
   "outputs": [],
   "source": [
    "small_batch_size = 3\n",
    "input_batches, input_lengths, target_batches, target_lengths = random_batch(small_batch_size)\n",
    "\n",
    "print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
    "print('target_batches', target_batches.size()) # (max_len x batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFNjmpOZjM_Z"
   },
   "source": [
    "Twórz modele o małym rozmiarze (można je kontrolować wzrokowo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nS0S6358jM_c"
   },
   "outputs": [],
   "source": [
    "small_hidden_size = 8\n",
    "small_n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers)\n",
    "decoder_test = LuongAttnDecoderRNN('general', small_hidden_size, output_lang.n_words, small_n_layers)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    decoder_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJLRqxZQjM_g"
   },
   "source": [
    "Aby przetestować koder, przeprowadź wsad wejściowy, aby uzyskać poszczególne wyjścia kodera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1525693682902,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "_biyc0SPjM_h",
    "outputId": "31da2c7f-ab9e-454a-b003-cc803836de53"
   },
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths, None)\n",
    "\n",
    "print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "print('encoder_hidden', encoder_hidden.size()) # n_layers * 2 x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ambBXpSVjM_m"
   },
   "source": [
    "Następnie zaczynając od tokena SOS, uruchamiaj tokeny słowne przez dekoder, aby uzyskać każdy następny token słowa. Zamiast robić to z całą sekwencją, robi się to po jednym na raz, aby wspierać używanie własnych przewidywań do wykonania następnej prognozy. Będzie to jednorazowy krok w danym momencie, ale w porcjach na czas. Aby to zadziałało w przypadku krótkich sekwencji, wielkość partii będzie się zmniejszać za każdym razem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1525693685952,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "j6Vc_e9fjM_n",
    "outputId": "1d7bc29c-de5d-4c52-ab0e-9bff916f25e8"
   },
   "outputs": [],
   "source": [
    "max_target_length = max(target_lengths)\n",
    "\n",
    "# Prepare decoder input and outputs\n",
    "decoder_input = Variable(torch.LongTensor([SOS_token] * small_batch_size))\n",
    "decoder_hidden = encoder_hidden[:decoder_test.n_layers] # Use last (forward) hidden state from encoder\n",
    "all_decoder_outputs = Variable(torch.zeros(max_target_length, small_batch_size, decoder_test.output_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    decoder_input = decoder_input.cuda()\n",
    "\n",
    "# Run through decoder one time step at a time\n",
    "for t in range(max_target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attn = decoder_test(\n",
    "        decoder_input, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    all_decoder_outputs[t] = decoder_output # Store this step's outputs\n",
    "    decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "# Test masked cross entropy loss\n",
    "loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    target_batches.transpose(0, 1).contiguous(),\n",
    "    target_lengths\n",
    ")\n",
    "print('loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXDizgVajM_p"
   },
   "source": [
    "# Trenowanie\n",
    "\n",
    "## Definiowanie iteracji treningu\n",
    "\n",
    "Aby ćwiczyć, najpierw uruchamiamy zdanie wejściowe za pomocą kodera słowo po słowie i śledzimy każde wyjście i ostatni stan ukryty. Następnie dekoder otrzymuje ostatni ukryty stan dekodera jako swój pierwszy ukryty stan, a token \"<SOS>\" jako pierwsze wejście. Stąd przechodzimy do przepowiadania następnego tokena z dekodera.\n",
    "\n",
    "### \"Wymuszanie nauczycieli\" i planowe pobieranie próbek\n",
    "\n",
    "\"Wymuszanie nauczycieli\" lub pobieranie próbek o największej wiarygodności oznacza wykorzystanie rzeczywistych wyników docelowych jako każdego następnego wejścia podczas treningu. Alternatywą jest użycie własnego odgadnięcia przez dekoder jako następnego wejścia. Korzystanie z wymuszania nauczyciela może spowodować szybsze zbieganie się sieci, ale [kiedy wyszkolona sieć jest wykorzystywana, może wykazywać niestabilność](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
    "\n",
    "Możesz obserwować wyniki wymuszonych przez nauczyciela sieci, które czytają ze spójną gramatyką, ale oddalają się od prawidłowego tłumaczenia - możesz myśleć o tym, że nauczyłeś się słuchać instrukcji nauczyciela, nie ucząc się samodzielnego wyjścia.\n",
    "\n",
    "Rozwiązanie problemu \"wymuszania\" nauczyciela jest znane jako [Zaplanowane próbkowanie](https://arxiv.org/abs/1506.03099), które po prostu przełącza się między wartościami docelowymi i przewidywanymi wartościami podczas trenowania. My losowo wybieramy użycie wymuszania nauczyciela instrukcją if podczas treningu - czasami będziemy podawać rzeczywisty cel jako wejście (ignorując wyjście dekodera), czasami użyjemy wyjścia dekodera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LkxmrG0GjM_q"
   },
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), ec, dc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fm2GZxu2jM_s"
   },
   "source": [
    "## Uruchamianie treningu\n",
    "\n",
    "Gdy wszystko jest gotowe, możemy zainicjować sieć i rozpocząć trening.\n",
    "\n",
    "Na początek uruchamiamy modele, optymalizatory i funkcję utraty (kryterium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y8QsdydpjM_s"
   },
   "outputs": [],
   "source": [
    "# Configure models\n",
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 50 # 1600  # 100\n",
    "# batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 1000 # 50000\n",
    "epoch = 0\n",
    "plot_every = 10\n",
    "print_every = 50  # 100\n",
    "evaluate_every = 50 # 1000\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mEGr4sQnYXSG"
   },
   "outputs": [],
   "source": [
    "from torchnet.logger import VisdomPlotLogger, VisdomLogger, VisdomTextLogger\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Start a job\n",
    "\n",
    "class Job:\n",
    "    def __init__(self, name, params={}, hostname='localhost', port=8890, n_epochs=10):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.hostname = hostname\n",
    "        self.port = port\n",
    "        self.avg_train_loss_logger = VisdomPlotLogger(\n",
    "        'line', port=port, opts={'title': f'{name}_Train Loss'})\n",
    "        self.job_logger = VisdomTextLogger(port=port, opts={'title': f'{name}_Job Info'})\n",
    "        self.log_logger = VisdomTextLogger(port=port, opts={'title': f'{name}_Train Loss Log'}, update_type='APPEND')\n",
    "        self.pbar = tqdm(total=n_epochs, ncols=60, mininterval=1.0, ascii=True, file=io.StringIO())\n",
    "        self.progress_logger = VisdomTextLogger(port=port, opts=dict(title=f'{name}_Progress Bar',\n",
    "            width=500,\n",
    "            height=50,\n",
    "        ))\n",
    "\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        j_str = json.dumps(self.params, indent=4)\n",
    "        j_str = j_str.replace('\\n', '<br/>\\n').replace(' ', '&nbsp;')\n",
    "        self.job_logger.log(f'{j_str}')\n",
    "\n",
    "        self.job_id = '#job_id'# body['id']\n",
    "        print(\"Starting job %s at %s\" % (self.job_id, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())))\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.log_every = 50\n",
    "        self.plot_every = 50\n",
    "        self.loss_avg = 0\n",
    "\n",
    "    def stop(self, ):\n",
    "        j = {'status': 'done'}\n",
    "        print(j)\n",
    "\n",
    "    def log(self, l):\n",
    "        def time_since(since):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "        self.log_logger.log(l)\n",
    "\n",
    "    def plot(self, x, y):\n",
    "        self.avg_train_loss_logger.log(x, y)\n",
    "\n",
    "    def record(self, e, loss):\n",
    "        self.loss_avg += loss\n",
    "\n",
    "        if e > 0 and e % self.log_every == 0:\n",
    "            self.log('(%s) %.4f' % (e, loss))\n",
    "\n",
    "        if e > 0 and e % self.plot_every == 0:\n",
    "            self.plot(e, self.loss_avg / self.plot_every)\n",
    "            self.loss_avg = 0\n",
    "\n",
    "    def update_progress(self, ):\n",
    "        self.pbar.update(1)\n",
    "        text = f'{self.pbar}'.replace(' ', '<font color=\"white\">#</font>')\n",
    "        self.progress_logger.log(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1525694418903,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "N2cXAJPC-Epg",
    "outputId": "3d0a8f26-5783-40ef-c1e5-757d116af3d7"
   },
   "outputs": [],
   "source": [
    "job = Job('seq2seq-translate', {\n",
    "    'attn_model': attn_model,\n",
    "    'n_layers': n_layers,\n",
    "    'dropout': dropout,\n",
    "    'hidden_size': hidden_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'clip': clip,\n",
    "    'teacher_forcing_ratio': teacher_forcing_ratio,\n",
    "    'decoder_learning_ratio': decoder_learning_ratio,\n",
    "}, n_epochs=n_epochs)\n",
    "job.plot_every = plot_every\n",
    "job.log_every = print_every\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "import visdom\n",
    "\n",
    "vis = visdom.Visdom(port=job.port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MDT8rrLFjM_u"
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMjoLgjRjM_v"
   },
   "source": [
    "# Ewaluacja sieci\n",
    "\n",
    "Ewaluacja jest w większości taka sama jak w przypadku treningu, ale nie ma żadnych wyników. Zamiast tego zawsze podajemy prognozy dekodera z powrotem do siebie. Za każdym razem, gdy przewiduje słowo, dodajemy je do ciągu wyjściowego. Jeśli przewiduje to token EOS, zatrzymujemy się tam. Przechowujemy także wyniki koncentracji dekodera dla każdego kroku, aby wyświetlić później.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1525694021482,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "cZyepm7XjM_v",
    "outputId": "57c71af0-b88c-4a96-85ce-02919fcf156f"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq, max_length=MAX_LENGTH):\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_lengths = [len(input_seqs[0])]\n",
    "\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "\n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "\n",
    "    # Run through encoder\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token])) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "\n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyY9aX0mjM_y"
   },
   "source": [
    "Możemy oceniać losowe zdania z zestawu treningowego i drukować dane wejściowe, docelowe i wyjściowe w celu dokonania subiektywnej oceny jakości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CpGS1t6AjM_z"
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVBji1kAjM_1"
   },
   "source": [
    "# Wizualizacja koncentracji\n",
    "\n",
    "Użyteczną właściwością mechanizmu koncentracji są jego wysoce interpretowalne wyniki. Ponieważ jest on używany do ważenia określonych wyjść kodera sekwencji wejściowej, możemy sobie wyobrazić, gdzie najlepiej jest skupić się w sieci na każdym kroku.\n",
    "\n",
    "Możesz po prostu uruchomić `plt.matshow(attentions)`, aby zobaczyć wyniki koncentracji wyświetlane jako matryca, z kolumnami będącymi krokami wejściowymi i wierszami będącymi krokami wyjściowymi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "73IB60mJjM_1"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-M099fekjM_3"
   },
   "source": [
    "Aby uzyskać lepsze wrażenia wizualne, wykonamy dodatkową pracę dodawania osi i etykiet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vM8VYNy5d-BX"
   },
   "outputs": [],
   "source": [
    "def show_plot_visdom():\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    attn_win = 'attention (%s)' % hostname\n",
    "    vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bjgNvhWrjM_3"
   },
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    mpl.style.use('default')\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)    \n",
    "    cax = ax.matshow(attentions.numpy(), cmap='jet')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "46Dts5BjjM_5"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    \n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "    # Show input, target, output text in visdom\n",
    "    win = 'evaluted (%s)' % hostname\n",
    "    text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "    vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-BmvDLCjM_6"
   },
   "source": [
    "#Kładąc wszystko razem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "od07gsWkjM_7"
   },
   "source": [
    "Żeby przeprowadziwy właściwy trening, wielokrotnie wywołujemy funkcję treningową.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1640
    },
    "colab_type": "code",
    "id": "0ahWiXo5jM_7",
    "outputId": "db42fd80-c6b2-4c76-c9d7-ba254da12a91"
   },
   "outputs": [],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random_batch(batch_size)\n",
    "\n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer, criterion\n",
    "    )\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "    \n",
    "    job.record(epoch, loss)\n",
    "    job.update_progress()\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'{job.pbar}')\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        ecs_win = 'encoder grad (%s)' % hostname\n",
    "        dcs_win = 'decoder grad (%s)' % hostname\n",
    "        vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "        vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpEnUGovjNAA"
   },
   "source": [
    "## Wyrysowywanie strat treningowych\n",
    "\n",
    "Wykreślanie odbywa się za pomocą matplotlib, przy użyciu macierzy `plot_losses`, która została utworzona podczas treningu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1525694357743,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "E4uxpB9ljNAB",
    "outputId": "a3a38cff-f32d-4a48-bccb-bf76b27332b2"
   },
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1525694358451,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "TQFXl0RuYXSs",
    "outputId": "418346de-43ef-424c-e357-73923e1a1ad4"
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 3  # 5\n",
    "xxx=[print(random.choice(pairs)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1525694367692,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "vQoeHfjfjNAC",
    "outputId": "b3fd14e8-1797-4f11-eead-9b5363485197"
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\"it's time for lunch .\")\n",
    "plt.matshow(attentions.numpy())\n",
    "show_plot_visdom()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "raw",
    "id": "yt5xbOUMYXSy"
   },
   "source": [
    "['i failed after all .', 'nie udało mi się jednak .']\n",
    "['i like you very much .', 'bardzo cię lubię .']\n",
    "[\"i can't leave .\", 'nie mogę wyjść .']\n",
    "['i feel very good .', 'czuję się bardzo dobrze .']\n",
    "[\"we've been here an hour .\", 'jesteśmy tutaj godzinę .']\n",
    "[\"i don't like it .\", 'nie lubię tego .']\n",
    "['tom likes boston .', 'tom lubi boston .']\n",
    "['that works .', 'to działa .']\n",
    "['i will try it again .', 'spróbuję ponownie .']\n",
    "['tom drank some beer .', 'tom wypił piwo .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1525694373607,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "Mb-wKK0yjNAE",
    "outputId": "3667f910-b880-4b8f-8fe1-8d819d54b6cc"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention('i failed after all .', 'nie udało mi się jednak .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1525694388434,
     "user": {
      "displayName": "Wojtek Czarnowski",
      "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEig/wmURGqbv_J8/s50-c-k-no/photo.jpg",
      "userId": "115130698336476923651"
     },
     "user_tz": -120
    },
    "id": "MjIOvXDajNAG",
    "outputId": "7ea18695-dbc5-4330-ff8d-7b5bdaa07ad4"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention('i like you very much .', 'bardzo cię lubię .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KygYF6nwjNAH",
    "outputId": "6804035f-6a49-47fc-da7c-d906b8270fd8"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"i can't leave .\", 'nie mogę wyjść .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2hL33p-mjNAK",
    "outputId": "e0f9e12a-3e21-44ab-9dba-4937ea9ef271"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention('i feel very good .', 'czuję się bardzo dobrze .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QE9BdY7RjNAL",
    "outputId": "26621b5e-af0d-4635-b66f-98616f7c6d42"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"we've been here an hour .\", 'jesteśmy tutaj godzinę .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8qyorxdcjNAM",
    "outputId": "06852f27-c699-4d83-d9f1-5e6613a8da1d"
   },
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"i don't like it .\", 'nie lubię tego .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapis modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: encoder, decoder, vocab:lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah data/seq2seq/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "dataset_path = Path('data/seq2seq')\n",
    "tmp_path = dataset_path / 'tmp/'\n",
    "!mkdir -p $tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB, ENCODER, DECODER = ['vocab', 'encoder', 'decoder']\n",
    "fn_trn1 = {VOCAB: 'eng-pol.vocab.27450pairs.p', \\\n",
    "           ENCODER: 'batched.encoder.h500.l2.e50000.gpu.torch', \\\n",
    "           DECODER: 'batched.decoder.h500.l2.e50000.gpu.torch'\n",
    "          }\n",
    "fn_dict = fn_trn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.save(encoder, tmp_path / fn_dict[ENCODER])\n",
    "torch.save(decoder, tmp_path / fn_dict[DECODER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "\n",
    "vocab = {'input_lang': input_lang, 'output_lang': output_lang}\n",
    "\n",
    "pickle.dump(vocab, open(tmp_path / fn_dict[VOCAB], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls -lah $tmp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zBBVHQXjNAS"
   },
   "source": [
    "# Ćwiczenia\n",
    "\n",
    "* Spróbuj użyć innego zestawu danych\n",
    "     * Kolejna para językowa\n",
    "     * Człowiek &rarr; Maszyna (np. Komendy IOT)\n",
    "     * Czat &rarr; Odpowiedź\n",
    "     * Pytanie &rarr; Odpowiedź\n",
    "* Zastąp zastosowane osadzenie przetrenowanymi wcześniej osadzeniami, takimi jak word2vec lub GloVe\n",
    "* Wypróbuj więcej warstw, więcej ukrytych jednostek i więcej zdań. Porównaj czas trenowania i wyniki.\n",
    "* Jeśli używasz pliku tłumaczenia, w którym pary mają dwie takie same frazy (`I am test \\t I am test`), możesz użyć tego jako autoencoder. Spróbuj tego:\n",
    "     * Trenuj jako autoencoder\n",
    "     * Zapisz tylko sieć kodera\n",
    "     * Wytrenuj nowy dekoder do tłumaczenia z tego miejsca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yHtNHu50jM_O",
    "8zBBVHQXjNAS"
   ],
   "default_view": {},
   "name": "seq2seq-translation-batched-wcz2.ipynb",
   "provenance": [
    {
     "file_id": "1LdwpSTwV_jrnw7vx0ponFH2q2vq2Nsug",
     "timestamp": 1524856859127
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
