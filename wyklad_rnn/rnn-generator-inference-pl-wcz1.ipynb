{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-generator-inference-wcz2-gpu.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1PMTT-NUS-yBYHz9iXOlYMYt-jTKgrwXF",
          "timestamp": 1524578004978
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5y5zxU-tMBqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inferencja w generatorze RNN przy użyciu GPU lub CPU"
      ]
    },
    {
      "metadata": {
        "id": "iQFiQEg2M7_L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "dataset_path = Path.home() / 'data/rnn_generator'; dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7tZoCQ87701",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8851ef5d-59e4-4906-baef-dcf899e3446b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795834067,
          "user_tz": -120,
          "elapsed": 1148,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE_GPU=True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gsxhdCiH1GH8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ALLCHARS, MODEL = ['all_characters', 'model']\n",
        "\n",
        "fn_witkacy = {ALLCHARS: 'all_characters.p', MODEL: 'model.h100.l1.e2000.cpu.dat'}\n",
        "# fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h100.l1.e2000.cpu.torch'}\n",
        "# fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h100.l1.e101.gpu.torch'}\n",
        "fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h400.l3.3000.gpu.torch'}\n",
        "\n",
        "\n",
        "fn_dict = fn_pan_tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fqi3uGN3XJR3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75125330-75ea-4b92-f90f-d75fea79c6a5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795835698,
          "user_tz": -120,
          "elapsed": 717,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tmp_path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data/rnn_generator/tmp')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "5O7rXy8lXM1l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "577b06f5-8d45-41c5-9f33-eb9c778c9090",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795837458,
          "user_tz": -120,
          "elapsed": 1724,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah $tmp_path"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8.5M\r\n",
            "drwxr-xr-x 2 root root 4.0K May  8 16:02 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
            "drwxr-xr-x 3 root root 4.0K May  8 13:19 \u001b[01;34m..\u001b[0m/\r\n",
            "-rw-r--r-- 1 root root  743 May  8 16:02 all_characters.pan_tadeusz.p\r\n",
            "-rw-r--r-- 1 root root 8.5M May  8 16:02 pan_tadeusz.h400.l3.3000.gpu.torch\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "abA8YbvTM9P7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ładowanie listy all_characters"
      ]
    },
    {
      "metadata": {
        "id": "-g3yLkRlM8Ci",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a0e581d-3f0d-4dcf-e021-be1ba0b8c07c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795838174,
          "user_tz": -120,
          "elapsed": 705,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "all_characters = pickle.load( open( tmp_path / fn_dict[ALLCHARS], 'rb' ) ); print(all_characters[:10])\n",
        "n_characters = len(all_characters); print(n_characters)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '%', \"'\", '(', ')', ',', '-']\n",
            "89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "klVO2LLbMxHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Kod"
      ]
    },
    {
      "metadata": {
        "id": "q3kvxKIPMzyC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2wurTvHMUsq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d4f255ed-5344-4d9f-cf10-bbe3922324f1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795842227,
          "user_tz": -120,
          "elapsed": 3203,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(to_gpu(tensor))\n",
        "\n",
        "print(char_tensor('ala ma kota'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 41\n",
            " 52\n",
            " 41\n",
            "  1\n",
            " 53\n",
            " 41\n",
            "  1\n",
            " 51\n",
            " 55\n",
            " 59\n",
            " 41\n",
            "[torch.cuda.LongTensor of size 11 (GPU 0)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tWUvpDlFMUvZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nW605sPbPDtf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7fd8ac23-ec21-4b7e-85be-41ad085671b5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795845340,
          "user_tz": -120,
          "elapsed": 1051,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 300 # 3000\n",
        "print_every = 100\n",
        "plot_every = 1\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "print(decoder, flush=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (encoder): Embedding(89, 100)\n",
            "  (gru): GRU(100, 100)\n",
            "  (decoder): Linear(in_features=100, out_features=89)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "448Y72-nQBtp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wczytywanie modelu"
      ]
    },
    {
      "metadata": {
        "id": "XLY4aFklQAqt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d9595319-de20-4659-a315-a70239e301c7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795846408,
          "user_tz": -120,
          "elapsed": 1021,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_path_cpu = tmp_path / fn_dict[MODEL]\n",
        "decoder = torch.load(model_path_cpu)\n",
        "print(decoder, flush=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (encoder): Embedding(89, 300)\n",
            "  (gru): GRU(300, 300, num_layers=4)\n",
            "  (decoder): Linear(in_features=300, out_features=89)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fmg9fOb2oggW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "decoder.gru.flatten_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFGkMgTMO9g8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generowanie tekstu"
      ]
    },
    {
      "metadata": {
        "id": "eDlhLm5M34yQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# priming strings\n",
        "# pan_tadeusz: kon, bę, Tad, Tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIM7xrfOMU0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "06747b23-8bb5-43a8-ac3b-22bb576a91ff",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795853115,
          "user_tz": -120,
          "elapsed": 3911,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Tadeusz', 1000, temperature=0.8))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tadeusz niego s dzima s czy pięcy.\n",
            "\n",
            "\n",
            "Wolich twrzelém,\n",
            "I stani w do korzanbie.\n",
            "\n",
            "\n",
            "Tarunicz czwéj\n",
            "Na się o kodziona po w balą w s czystaną stowałka mogał jasić żalo, dzi zsię, postawa, na się sią udzico rzet czarta pod wszysich się powanie do zusła zielżą daze nadzy kosta naraz swana z biędzigą ukał,\n",
            "Tam zą nież jtak znesznia spowada poczy na muszela,\n",
            "Rzepy, zagopadzać żaga,\n",
            "Doznie zanął sani niegch garz sza, rzy zusza ogastech do buży ubał nan sprzykli\n",
            "Ddoweby siel ocz udziości bachzął;\n",
            "Wypnionaniem.\n",
            "\n",
            "Zawanie zwaraz postrzeszać poz pan zaloszanie ma znierząc stadził.\n",
            "\n",
            "Tu strzyskrowo przyszczyźnu nal dobie nopaślu ja zmostu,\n",
            "«Apasi grzeniwać w lade z żylku oszego upo naganowy do kranik,\n",
            "Dłurzyk go dostali sywyszciém ciet teże, pon polkia lachniem niby jak  go przennny;\n",
            "Wy loszczych z wzyn buka że --\n",
            "A roszieł wszykać wiego w nie sunnia s ba pana,\n",
            "Czydzia,\n",
            "Redzieni naranie Gadzi, w w jak w pogować jaszy\n",
            "W usta zasił doże  pobanie, pogał puszy!\n",
            "I się płowiemczynie, Coże poru na nie banie nie sient\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mIU6dyzRMU25",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "8c7ab88d-b65b-4f73-e1f6-30fde326e1c0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795856254,
          "user_tz": -120,
          "elapsed": 3126,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Tad', 1000, temperature=0.5))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tadziem w rosta miele się się zbosta trzy za krzeszczynie go z w kodzukł bary się; stana się wieszyskać szanie w wz czyszczele;\n",
            "Rawarząd powielano:\n",
            "Tadzie w ponia,\n",
            "Cak się w tak puszki w rabiem spowanie postach w kola jak cz czartawać panie nieży po czanie posta się obaranie szeszył spostanie charaz na na s w w i sposkał sponie cać na usta spodzieli jaczawieni nie postani do był z za czawił jak stania wiele się jak, na z zanie przać obalki,\n",
            "Posta sprzyszył się się growieno częsty poszczy nie jesze stadzia od w sabym się w się i sieskie uskło chowie obył bary się krzel w cie się na marszczy stawie się s dale komiania postał krowska o czyski stawie z wieszczania z wnie,\n",
            "Po wszeski w suby,\n",
            "Dosta pochu dosko jak tak odwieszcze z na koraznowiem s siedzie obarał się ma pyle stadzie piędziejo w kroducho pod w grzelanie s pobozuwie podwanie,\n",
            "Na gowi się posz z wszysze mał się obarawa,\n",
            "A sta pod i się wsta spony,\n",
            "Jest na poganie na na głowiem,\n",
            "Do zabie się poli pieposzyć spowawie na się się się, t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OPYzLn5jMU5x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bd07af1f-cfc6-4d1b-9148-11076f0ff506",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795858105,
          "user_tz": -120,
          "elapsed": 1839,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('bę', 200, temperature=1.4))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bęmiém zoły przeżówsz kojsko weknóżny\n",
            "Ota nibuwsło wiy!\n",
            "Apokugo Wozawiwa neszcia_ i rzyja dżajem,\n",
            "Ka.\n",
            "ĘMieją, datcoje ż żawery,\n",
            "Agki okyć na wssapokłastych zażę:\n",
            "Udszesorzęcby\n",
            "Kdzas dzieniéj w zuskowszy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIsFkxsFMVBS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}