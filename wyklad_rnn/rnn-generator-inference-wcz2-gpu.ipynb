{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn-generator-inference-wcz2-gpu.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1PMTT-NUS-yBYHz9iXOlYMYt-jTKgrwXF",
          "timestamp": 1524578004978
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5y5zxU-tMBqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RNN generator inference, GPU or CPU"
      ]
    },
    {
      "metadata": {
        "id": "iQFiQEg2M7_L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "dataset_path = Path.home() / 'data/rnn_generator'; dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7tZoCQ87701",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc65b5e2-7672-4a5f-eb9a-24d558787134",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640024628,
          "user_tz": -120,
          "elapsed": 557,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE_GPU=True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gsxhdCiH1GH8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ALLCHARS, MODEL = ['all_characters', 'model']\n",
        "\n",
        "fn_witkacy = {ALLCHARS: 'all_characters.p', MODEL: 'model.h100.l1.e2000.cpu.dat'}\n",
        "# fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h100.l1.e2000.cpu.torch'}\n",
        "# fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h100.l1.e101.gpu.torch'}\n",
        "fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h400.l2.3000.gpu.torch'}\n",
        "\n",
        "\n",
        "fn_dict = fn_pan_tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "abA8YbvTM9P7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load all_characters list"
      ]
    },
    {
      "metadata": {
        "id": "-g3yLkRlM8Ci",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32cd7b60-c16d-42ca-f5e9-015d3dae7256",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640026787,
          "user_tz": -120,
          "elapsed": 1427,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "all_characters = pickle.load( open( tmp_path / fn_dict[ALLCHARS], 'rb' ) ); print(all_characters[:10])\n",
        "n_characters = len(all_characters); print(n_characters)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '%', \"'\", '(', ')', ',', '-']\n",
            "89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "klVO2LLbMxHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code"
      ]
    },
    {
      "metadata": {
        "id": "q3kvxKIPMzyC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2wurTvHMUsq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "87e56d04-0861-4493-c6df-c9683b8d0c41",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640032259,
          "user_tz": -120,
          "elapsed": 3130,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(to_gpu(tensor))\n",
        "\n",
        "print(char_tensor('ala ma kota'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 41\n",
            " 52\n",
            " 41\n",
            "  1\n",
            " 53\n",
            " 41\n",
            "  1\n",
            " 51\n",
            " 55\n",
            " 59\n",
            " 41\n",
            "[torch.cuda.LongTensor of size 11 (GPU 0)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tWUvpDlFMUvZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nW605sPbPDtf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 300 # 3000\n",
        "print_every = 100\n",
        "plot_every = 1\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "print(decoder, flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "448Y72-nQBtp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ]
    },
    {
      "metadata": {
        "id": "XLY4aFklQAqt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "7b3dfb65-62c3-46cf-9b47-ce04e47f041e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640035189,
          "user_tz": -120,
          "elapsed": 661,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_path_cpu = tmp_path / fn_dict[MODEL]\n",
        "decoder = torch.load(model_path_cpu)\n",
        "print(decoder, flush=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (encoder): Embedding(89, 400)\n",
            "  (gru): GRU(400, 400, num_layers=2)\n",
            "  (decoder): Linear(in_features=400, out_features=89)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fmg9fOb2oggW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "decoder.gru.flatten_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFGkMgTMO9g8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ]
    },
    {
      "metadata": {
        "id": "eDlhLm5M34yQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# priming strings\n",
        "# pan_tadeusz: kon, bę, Tad, Tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIM7xrfOMU0S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "c0849789-08d7-4726-e3ef-b0865baa6f7b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640236115,
          "user_tz": -120,
          "elapsed": 2823,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Tadeusz', 1000, temperature=0.8))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tadeusz ruce bysię zięcze wyką potaniu.\n",
            "\n",
            "                      Niem połowie,\n",
            "W korzał sługi trzele cocałem połują,\n",
            "Podłużny, promiéj w krzykniut tak pióre,\n",
            "Kostarskiém okonatała włachał który broda.\n",
            "\n",
            "\n",
            "Bóż tę go pola parygie  rozmadali w czy Kostrzyła nie trono,\n",
            "Stkowały myśliwycz w górą w kwitro ow rozyłu wrzałby i tylkaczy mował.\n",
            "\n",
            "Wyba pocią piękiem utę piękrowaz nach natwotą wszystkiém, parte kielarz w goście\n",
            "Na na to wławę w romadził karoki,\n",
            "A taki w gości, i tylko z domu niech tali\n",
            "Perna dą ba koło mu Aść kiedą kiedyń kut w remstaz tu tu,\n",
            "I cię mił i rzezka kotyk,\n",
            "A zoczasiem, patrze, rączą ktowytem takoły,\n",
            "Aby drugi -- Sugrzy rował Wogruzechie,\n",
            "Gt dardowszy oba zieśmie już zachłogą,\n",
            "Parkusy z rogardość latem wyrzymowałby puszczeniéj Kodmercyfu utrzał\n",
            "Że kamieni, popowiatrowie\n",
            "\n",
            "Zowałamiał to gurł wszedwiadał go cię kwezowny,\n",
            "Szął włos zdronie,\n",
            "Tadeusz w zatrępiona,\n",
            "Dostał Pan Sędzie Kuścich którdego w patrzał w stołowa zawrócił rupatem.\n",
            "\n",
            " ktroci, ztéj głowby,\n",
            "Szczadać na który i gościł bo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mIU6dyzRMU25",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "27b9173f-4a1e-4932-e5d0-f8d5a2f257d0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640216337,
          "user_tz": -120,
          "elapsed": 3036,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Tad', 1000, temperature=0.5))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tadaladzie wyrzadzał wyrowie ziemię w toł kołowy\n",
            "Stolach w okomna, trobaskiém Kozrokiem.\n",
            "\n",
            "        cztart w ciągnął, krosi takiem,\n",
            "Podania takie tarze w kapiastwi wrzał gościał\n",
            "Przecież kto więto w tali wielkiéj koły w wieść u ogromował,\n",
            "Jak tylko kołową wię karku Domeykie pańskił ziemiał oprawił\n",
            "I w krwawy w tylecie i za wytarzą krowa,\n",
            "Tada w koło, tak zatrał z nim zagrodzie.\n",
            "\n",
            "                                                                                                                                                                                                                                                 Pan tém wyrzecie powierzeciał i wielki wyrzekł w tak,\n",
            "A drzwi wyrzawie poworze powiatem,\n",
            "A do tylko z Aśliwszy urzą krzykniemi.\n",
            "\n",
            "\n",
            "\n",
            "Tam pa przysię w kołowy,\n",
            "Dowada i się wię cię w tył w koleniu,\n",
            "Przez gadał puszcie paroskim, rodku potowy\n",
            "Szaka Litwę w nim do Litawie wyrzekł kuturom,\n",
            "Który kołował w komny niedźwieciego trwał w goście rozmał gości.\n",
            "\n",
            "\n",
            "\n",
            "Stanął Kuż krozał do włarny kar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OPYzLn5jMU5x",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "27a19960-a8fd-4851-c34f-b55620712a23",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524640184048,
          "user_tz": -120,
          "elapsed": 1137,
          "user": {
            "displayName": "Wojtek Czarnowski",
            "photoUrl": "//lh6.googleusercontent.com/-Sx8k456RbyI/AAAAAAAAAAI/AAAAAAAAEVg/Bn2jWaJ_hM0/s50-c-k-no/photo.jpg",
            "userId": "115130698336476923651"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('bę', 200, temperature=1.4))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bę Aśoramisko;a\n",
            "Po wiórnieeru somanaą niezas Dzykncowywyz, nióbł bręku jéj, zoczysłać, lepiwił Suknę lilicsty,\n",
            "Parnałem Kasrzegu.\n",
            "PaDezanąszdał Wę, Nigadrą nagu.\n",
            "Gymiał Gdziaste, -- N czystawiny,\n",
            "k? Tie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIsFkxsFMVBS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}