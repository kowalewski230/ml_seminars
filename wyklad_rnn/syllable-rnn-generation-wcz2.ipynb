{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "syllable-rnn-generation-wcz2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1m8m5mSoiMrgPWvQjFcQCudYJWF9mR6Qq",
          "timestamp": 1530337993718
        },
        {
          "file_id": "1DztHxtFb_tfNmrLeOcav9myCNqeTlZ82",
          "timestamp": 1524492157904
        },
        {
          "file_id": "1sv_cPvPEWEOrG9wS3fPjAOj-Wvs6kc0P",
          "timestamp": 1524467882593
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iVsYe7S5XM5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/eBRPvWB.png)\n",
        "\n",
        "# Generowanie poezji za pomocą RNN i PyTorch sylabami\n",
        "\n",
        "[W tutorialu](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) użyliśmy RNN, aby sklasyfikować nazwiska znak po znaku. Tym razem wygenerujemy tekst sylaba po sylabie.\n",
        "```\n",
        "Litwo! Ojczyzno moja! ty jesteś klucz wyziemu, \n",
        "To opugo cząciły tak lasu czeleta. \n",
        "Choć nie będzie mowę świeci się za tém, \n",
        "A Dozgon++ na Litwę przerzucił w okolicy, \n",
        "Dosyć się opicie przyciągnąć w pałacu; \n",
        "\n",
        "Tamdzini nawet mimo osobnych ogórki. \n",
        "Choć zwyciętunia, mimo pukle wyślą, \n",
        "Odemknął, wbiegł wyszedł, pewnie miłośnik łowił. \n",
        "Bo przekorza, i skrobiąc nabój do Warszawy. \n",
        "Dość co oddało plecie tak fawował, \n",
        "A tam się cukier wytaczać na nich wybująca. \n",
        "\n",
        "```\n",
        "\n",
        "Ok, możesz zadać sobie pytanie, czy ten tutorial jest rzeczywiście praktyczny? A jednak jest, tego rodzaju modele generatywne stanowią fundament tłumaczenia maszynowego, opisywania obrazów, generowania odpowiedzi na pytania i wielu innych zastowań.\n",
        "\n",
        "Zobacz [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) żeby nauczyć się więcej w tym temacie."
      ]
    },
    {
      "metadata": {
        "id": "WamMk47AXM5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Polecana lektura\n",
        "\n",
        "Zakładam że jest już zainstalowany PyTorch, znasz Python'a, oraz znasz pojęcie Tensor'ów:\n",
        "\n",
        "* http://pytorch.org/ - instalacja PyTorch\n",
        "* [Deep Learning with PyTorch: A 60-minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) - Podstawy PyTorch\n",
        "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) przykłady wykorzystania PyTorch\n",
        "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) jeżeli znasz Lua Torch\n",
        "\n",
        "Trochę wiedzy o RNN:\n",
        "\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) przykłady z życia wzięte\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) RNN i LSTM w pigułce\n",
        "\n",
        "Zobacz także podobne tutoriale z serii:\n",
        "\n",
        "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) używa RNN do klasyfikacji\n",
        "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) opierając się na tym modelu, dodaje kategorię jako dane wejściowe"
      ]
    },
    {
      "metadata": {
        "id": "g8KbaMvrXM5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie korpusu"
      ]
    },
    {
      "metadata": {
        "id": "9wuZu6mE057C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plik wejściowy (korpus) to duży plik tekstowy. Zamieniamy duże litery na małe dodając token `_up_` lub `_cap_`, a potem dzielimy go na sylaby programem `stemmer`."
      ]
    },
    {
      "metadata": {
        "id": "IKPynzs-yyW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenizacja wielkich liter"
      ]
    },
    {
      "metadata": {
        "id": "80ZjgATAXsVn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "dataset_path = Path('data/rnn_generator'); dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'\n",
        "!mkdir -p $tmp_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPFXCZguyyXA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah $dataset_path/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eTRgNsMyyXD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
        "fn_corpus_caps = dataset_path/'pan_tadeusz.caps1.txt'\n",
        "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJtiF_yZiA-z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import re, string\n",
        "\n",
        "def do_caps(ss):\n",
        "  TOK_UP,TOK_CAP = ' _up_ ', ' _cap_ '\n",
        "  res = []\n",
        "  re_word = re.compile('\\w')\n",
        "  for s in re.findall(r'\\w+|\\W+', ss):\n",
        "      res += ([TOK_UP,s.lower()] if (s.isupper() and (len(s)>2))\n",
        "              else [TOK_CAP,s.lower()] if s.istitle()\n",
        "              else [s.lower()])\n",
        "  return ''.join(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5cAVEtNiC9X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "corpus_tmp = fn_corpus_char.open('r').read()\n",
        "corpus_tmp = do_caps(corpus_tmp)\n",
        "fn_corpus_caps.open('w').write(corpus_tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbMBLa_nxp2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Podział korpusu na sylaby"
      ]
    },
    {
      "metadata": {
        "id": "efaGn-TLyyXG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "platform_suffixes = {'Linux': 'linux', 'Darwin': 'macos'}\n",
        "import platform\n",
        "platform_suffix = platform_suffixes[platform.system()]\n",
        "stemmer_bin = f'LD_PRELOAD=\"\" bin/stemmer.{platform_suffix}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3qYJ20EyyXI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# !$stemmer_bin -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGbQYAUYyyXK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!$stemmer_bin -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_caps -o $fn_corpus_syl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpf6QibJyyXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Załadowanie do pamięci i tokenizacja"
      ]
    },
    {
      "metadata": {
        "id": "OSyGzgVD057R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ładujemy plik do pamięci i tokenizujemy. Tworzymy też listę wszystkich tokenów `all_tokens`. Mamy specjalne tokeny `_cap_` i `_up_`, zamieniamy znaki końca lini na token `_eol_` i dodajemy token `_unk_` na wypadek, gdybyśmy użyli sylaby (tokena), który nie wystąpił wcześniej w korpusie."
      ]
    },
    {
      "metadata": {
        "id": "g9H83p3sXM5L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "file = open(fn_corpus_syl).read()\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maVrQMtZyyXU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# taken from fastai/text.py\n",
        "import re, string\n",
        "\n",
        "# remove +,- chars from punctuation set to keep syllables e.g.'--PO++' intact\n",
        "# remove _ char to keep tokens intact\n",
        "punctuation=re.sub('[_\\+-]', '', string.punctuation)\n",
        "re_tok = re.compile(f'([{punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "\n",
        "def tokenize(s, repl_unk=True): \n",
        "  strings = re_tok.sub(r' \\1 ', s).replace('\\n', ' _eol_ ').split()\n",
        "  if repl_unk:\n",
        "    strings = [str2tok(s) for s in strings]\n",
        "  return strings\n",
        "\n",
        "file_tok = tokenize(file, repl_unk=False); len(file_tok), file_tok[:8]\n",
        "file_tok_len = len(file_tok)\n",
        "\n",
        "spec_tokens = ['_unk_', '_eol_', '_cap_', '_up_']\n",
        "\n",
        "all_tokens = []\n",
        "all_tokens.extend(spec_tokens)\n",
        "all_tokens.extend(sorted(list(set(file_tok))))\n",
        "n_tokens = len(all_tokens); print(n_tokens, all_tokens[:50])\n",
        "\n",
        "tok2idx_dict = {tok: idx for (idx, tok) in enumerate(all_tokens)}\n",
        "\n",
        "def str2tok(str) -> int:\n",
        "  return str if tok2idx_dict.get(str, 0) else all_tokens[0]\n",
        "\n",
        "def tok2idx(tok) -> int:\n",
        "  return tok2idx_dict.get(tok, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDQqpRYzxT4L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def str2syl2tok(text):  \n",
        "  fn_tmp_text_caps = Path(tmp_path / 'tmp_text_caps1.txt')\n",
        "  fn_tmp_text_syl = Path(tmp_path / 'tmp_text_syl1.txt')\n",
        "  \n",
        "  text = do_caps(text)\n",
        "  fn_tmp_text_caps.open('w').write(text)\n",
        "  \n",
        "  !$stemmer_bin -s 7683 -d bin/stemmer2.dic -i $fn_tmp_text_caps -o $fn_tmp_text_syl\n",
        "  \n",
        "  text_syl = fn_tmp_text_syl.open('r').read()\n",
        "  \n",
        "  # kill last \\n eol char possibly added by stemmer\n",
        "  if text_syl[-1] == '\\n':\n",
        "    text_syl = text_syl[:-1]\n",
        "\n",
        "  text_tok = tokenize(text_syl, repl_unk=True)\n",
        "    \n",
        "  return text_tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwnDdj75paVP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tekst = 'LITWO! Ojczyzno moja!\\nTy jesteś jak zdrowie.\\nIle cię trzeba cenić ble ble '\n",
        "tekst_tok = str2syl2tok(tekst)\n",
        "print(tekst_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lL_XOG-fXM5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aby stworzyć 'wejścia' z tego dużego ciągu danych, podzielimy go na kawałki po 400 sylab."
      ]
    },
    {
      "metadata": {
        "id": "_zwmRSAHXM5T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 400\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_tok_len - chunk_len -1)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file_tok[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAUKI80V6CvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Funkcje pomocnicze do dekodowania"
      ]
    },
    {
      "metadata": {
        "id": "JzAAi_95yyXq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def syl2str(a_list, delim='/'): \n",
        "  s = ' '.join(a_list)\n",
        "  \n",
        "  repl_list = [\n",
        "      ('++ --', delim), \n",
        "  ]\n",
        "  for repl in repl_list:\n",
        "    s = s.replace(repl[0], repl[1])\n",
        "  \n",
        "  return s\n",
        "\n",
        "print(syl2str(random_chunk()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUs-bnu5q2FF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def decode_tokens(e_str):\n",
        "  # decode _eol_, _cap_ and _up_\n",
        "  # leave _unk_ token alone\n",
        "  e_syl = e_str.split(' ')\n",
        "  e_syl2 = []\n",
        "\n",
        "  cap = False; up = False\n",
        "\n",
        "  for syl in e_syl:\n",
        "    if syl == '_eol_': syl = '\\n'\n",
        "\n",
        "    if syl not in ['_cap_', '_up_']:\n",
        "      if cap == True: syl = syl.title(); cap = False\n",
        "      if up == True: syl = syl.upper(); up = False        \n",
        "      e_syl2.append(syl)\n",
        "\n",
        "    if syl == '_cap_': cap = True\n",
        "    if syl == '_up_': up = True\n",
        "\n",
        "  return ' '.join(e_syl2)\n",
        "\n",
        "print(decode_tokens(syl2str(random_chunk(), delim=''))[:300])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqbQLawfrV-n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def fix_punctuation(s): \n",
        "  repl_list = [\n",
        "      ('\\n ', '\\n'), \n",
        "      (' ,', ','),\n",
        "      (' .', '.'),\n",
        "      (' !', '!'),\n",
        "      (' ?', '?'),\n",
        "      (' ;', ';'),\n",
        "      ('( ', '('),\n",
        "      (' )', ')'),\n",
        "      (' «', '«'),\n",
        "      ('» ', '»'),\n",
        "      (' :', ':')\n",
        "  ]\n",
        "  \n",
        "  for repl in repl_list:\n",
        "    s = s.replace(repl[0], repl[1])\n",
        "  \n",
        "  return s\n",
        "\n",
        "print(fix_punctuation(decode_tokens(syl2str(random_chunk(), delim='')))[:300])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnmXGTva5nw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie treningu"
      ]
    },
    {
      "metadata": {
        "id": "2cIY0S4g0gZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GPU?"
      ]
    },
    {
      "metadata": {
        "id": "2hTjsshick9K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5cFLx6WXM5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Budowa sieci rekurencyjnej\n",
        "\n",
        "Ten model przyjmie jako wejściie token dla kroku $ t _ {- 1} $ i ma wyprowadzić następny token $ t $. Istnieją trzy warstwy - jedna warstwa liniowa, która koduje znak wejściowy do stanu wewnętrznego, jedna warstwa GRU (która może sama mieć wiele warstw), która działa na tym stanie wewnętrznym i stanie ukrytym, oraz warstwa dekodera, która wyprowadza rozkład prawdopodobieństwa."
      ]
    },
    {
      "metadata": {
        "id": "tZ8chQcVXM5X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62XSRFgkXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensory wejściowe i docelowe"
      ]
    },
    {
      "metadata": {
        "id": "UaOPvn0rXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Każdy 'kawałek' zostanie przekształcony w tensor, a dokładnie w `LongTensor` (używany do wartości całkowitych), poprzez przepuszczenie wszystkich tokenów ciągu i wyszukiwanie indeksu każdej sylaby w `all_tokens`."
      ]
    },
    {
      "metadata": {
        "id": "q0H2nwMMXM5a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn token list into list of longs\n",
        "def tok_tensor(token_list):\n",
        "    tensor = torch.zeros(len(token_list)).long()\n",
        "    for c in range(len(token_list)):\n",
        "        tensor[c] = tok2idx(token_list[c])\n",
        "    \n",
        "    return Variable(to_gpu(tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrTEI7EjyyX2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tekst = 'Litwo! Ojczyzno moja! ty jesteś jak zdrowie;'\n",
        "tekst_tok = str2syl2tok(tekst)\n",
        "print(tekst_tok)\n",
        "# a_token_list = tekst_tok; print(a_token_list)\n",
        "print(tok_tensor(tekst_tok))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Su49JvFXM5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wreszcie możemy zmontować parę tensorów wejściowych i docelowych do treningu, z losowego kawałka. Wejściem zostaną wszystkie tokeny * aż do przedostatniego*, a celem (targetem) będą wszystkie tokeny * od drugiego*. Jeśli więc nasz kawałek to \"abc\", wejście będzie odpowiadać \"ab\", podczas gdy cel to \"bc\"."
      ]
    },
    {
      "metadata": {
        "id": "mLzzsbTRXM5d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = tok_tensor(chunk[:-1])\n",
        "    target = tok_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSJ_szQTXM5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ocena wyników\n",
        "\n",
        "Aby ocenić sieć, będziemy podawać po jednym tokenie na raz, wykorzystywać wyjścia sieci jako rozkład prawdopodobieństwa dla następnego znaku i powtarzać. Aby rozpocząć generowanie, przekazujemy ciąg wstępny, aby rozpocząć budowanie stanu ukrytego, z którego następnie generujemy po jednym tokenie na raz."
      ]
    },
    {
      "metadata": {
        "id": "2ecqC4rWXM5f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_tokl=[all_tokens[1]], predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = tok_tensor(prime_tokl)\n",
        "    predicted = list(prime_tokl)  # need a copy of the list\n",
        "\n",
        "    # Use priming token list to \"build up\" hidden state\n",
        "    for p in range(len(prime_tokl) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        \n",
        "        # in pytorch 0.4.0 max, min fail if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/6996\n",
        "        # in all pytorch versions multinomial fails if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/871\n",
        "        # temp fix, kill Infs and nans\n",
        "        # https://discuss.pytorch.org/t/how-to-set-inf-in-tensor-variable-to-0/10235\n",
        "        output_dist[output_dist == float(\"Inf\")] = 0\n",
        "        output_dist[output_dist == float(\"nan\")] = 0\n",
        "        \n",
        "        top_i = torch.multinomial(output_dist, 1)[0].item()\n",
        "        \n",
        "        # Add predicted token to the list and use as next input\n",
        "        predicted_token = all_tokens[top_i]\n",
        "        predicted.append(predicted_token)\n",
        "        inp = tok_tensor([predicted_token])\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZY0fPgEXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Trening sieci"
      ]
    },
    {
      "metadata": {
        "id": "-pM5T97tXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Funkcja pomocnicza do wydrukowania upływającego czasu:"
      ]
    },
    {
      "metadata": {
        "id": "hQnLeX-TXM5h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0pntPTWEXM5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Główna funkcja treningowa"
      ]
    },
    {
      "metadata": {
        "id": "QKb7-MeXXM5j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].expand(1))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2EiZtEsA0571",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Opcjonalny monitoring postępu treningu"
      ]
    },
    {
      "metadata": {
        "id": "HpYR9j3qyyYF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "USE_VISDOM = True\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "vis = None\n",
        "if USE_VISDOM:\n",
        "    import visdom\n",
        "    vis = visdom.Visdom(port=8890)\n",
        "\n",
        "def vis_update_line_chart(vis, name, x, y, first_step):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.line(Y=np.array([y]), X=np.array([x]), win=name, opts=dict(title=name),\n",
        "             update=None if first_step else 'append')\n",
        "\n",
        "def vis_update_text_win(vis, name, text):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.text(text, win=name, opts=dict(title=name), append=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFj5nGp5yyYI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class X(str):\n",
        "    def rpl(self, p, c='lightgray'):\n",
        "        return X(self.replace(p, f'<font color=\"{c}\">{p}</font>'))\n",
        "    def rpl2(self, p, p2):\n",
        "        return X(self.replace(p, p2))\n",
        "      \n",
        "def format_html(e_str):\n",
        "  return X(e_str).rpl('/').rpl('--', c='red').rpl('++', c='red').rpl2('\\n', '\\n<br/>')\n",
        "\n",
        "e_str = fix_punctuation(decode_tokens(syl2str(random_chunk(), delim='')))[:400]\n",
        "from IPython.core.display import display, HTML\n",
        "e_html = format_html(e_str); display(HTML(e_html))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tY2U1vt4yyYK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def bad_words(e_syl): e_str = syl2str(e_syl); return (e_str.count('++') + e_str.count('--')) / len(e_syl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyo69fakXM5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Następnie definiujemy parametry treningowe i rozpoczynamy trening:"
      ]
    },
    {
      "metadata": {
        "id": "ILThkaRPXM5k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 3000   # 2000\n",
        "plot_every = 50\n",
        "hidden_size = 500 # 100\n",
        "n_layers = 3 # 1\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_tokens, hidden_size, n_tokens, n_layers)\n",
        "if USE_GPU:\n",
        "    decoder.cuda()\n",
        "print(decoder, flush=True)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if USE_GPU:\n",
        "    criterion.cuda()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "all_bw = []\n",
        "bw_avg = 0\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "iterable = range(1, n_epochs + 1)\n",
        "tqdm_ = tqdm(iterable, '', leave=False, dynamic_ncols=True)\n",
        "first_step = True\n",
        "\n",
        "prime_tok = str2syl2tok('Litwo! Ojczyzno moja!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6y2kdZUj2bw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_every = 100\n",
        "\n",
        "for epoch in tqdm_:\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    # current loss chart\n",
        "    vis_update_line_chart(vis, 'loss', epoch, loss, epoch == 1)\n",
        "\n",
        "    # bad words    \n",
        "    bw = bad_words(evaluate(prime_tok, 100))\n",
        "    bw_avg += bw\n",
        "\n",
        "    # current bad words chart\n",
        "    vis_update_line_chart(vis, 'bad_words', epoch, bw, epoch == 1)\n",
        "    \n",
        "    # progress_bar\n",
        "    tqdm_.set_postfix({'loss': loss, 'bw': bw})\n",
        "    text = f'&nbsp;<font color=\"red\">{tqdm_}</font>'\n",
        "    vis_update_text_win(vis, 'progress_bar', text)\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        e_syl = evaluate(prime_tok, 1000)\n",
        "        e_bw = bad_words(e_syl)\n",
        "        stats_str = '\\n[%s (%d %d%%) loss=%.4f bw=%.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, e_bw)\n",
        "        print(stats_str)\n",
        "        \n",
        "        e_str = fix_punctuation(decode_tokens(syl2str(e_syl, delim='')))\n",
        "        e_html = format_html(e_str); display(HTML(e_html))\n",
        "        print(flush=True)        \n",
        "        \n",
        "        text = f'<b>{stats_str}</b><br />{e_html}'\n",
        "        vis_update_text_win(vis, 'evaluation', text)\n",
        "        \n",
        "        e_syl_path = tmp_path / 'e_syl.txt'\n",
        "        e_syl_path.open('w').write(' '.join(e_syl))\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        vis_update_line_chart(vis, 'loss_avg', epoch, loss_avg / plot_every, first_step)\n",
        "        vis_update_line_chart(vis, 'bad_words_avg', epoch, bw_avg / plot_every, first_step)\n",
        "        all_bw.append(bw)\n",
        "        bw_avg = 0\n",
        "        first_step = False\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXrPD94SXM5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Kreślenie wartości straty\n",
        "\n",
        "Wykreślanie historii straty z `all_losses` pokazuje uczenie sieci:"
      ]
    },
    {
      "metadata": {
        "id": "meKCxPo3XM5n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('default')\n",
        "mpl.style.use('bmh')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WabkwPqZ5WlH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Zapis i odczyt sieci"
      ]
    },
    {
      "metadata": {
        "id": "EtK-9BuuB-GX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Zapisanie sieci"
      ]
    },
    {
      "metadata": {
        "id": "aSA3Utpc6dXV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs=4004\n",
        "\n",
        "ALLTOKS, MODEL = ['all_tokens', 'model']\n",
        "fn_pan_tadeusz = {ALLTOKS: f'all_tokens.n{n_tokens}.pan_tadeusz.p', \n",
        "                  MODEL: f'pan_tadeusz.h{hidden_size}.l{n_layers}.e{n_epochs}.gpu.torch'}\n",
        "fn_dict = fn_pan_tadeusz; fn_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ-j5_4I6d0g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save all_tokens\n",
        "import pickle\n",
        "all_tokens_path = tmp_path / fn_dict[ALLTOKS]\n",
        "pickle.dump(all_tokens, open(all_tokens_path, 'wb'))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# save model\n",
        "model_path = tmp_path / fn_dict[MODEL]\n",
        "torch.save(decoder, model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dt8_7GshWi4M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah $tmp_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHbGgVULB_cp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "decoder.state_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BalhBXFKfErp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Załadowanie sieci"
      ]
    },
    {
      "metadata": {
        "id": "-D9_ONXNMHPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  all_tokens_path = tmp_path / fn_dict[ALLTOKS]\n",
        "  print(f'all_tokens_path = {all_tokens_path}')\n",
        "  all_tokens = pickle.load(open(all_tokens_path, 'rb'))\n",
        "  n_characters = len(all_tokens)\n",
        "  tok2idx_dict = {tok: idx for (idx, tok) in enumerate(all_tokens)}\n",
        "\n",
        "  model_path = tmp_path / fn_dict[MODEL]\n",
        "  decoder = torch.load(model_path)\n",
        "  print(f'model_path = {model_path}')\n",
        "  print(decoder.state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fEFWi-FXM5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ewaluacja w różnych \"temperaturach\"\n",
        "\n",
        "W powyższej funkcji `evaluate`, za każdym razem, gdy dokonywana jest prognoza, wyjścia są dzielone przez przekazany argument \"temperature\". Użycie większej liczby sprawia, że wszystkie akcje są bardziej jednakowo prawdopodobne, a tym samym dają nam \"bardziej losowe\" wyniki. Użycie mniejszej wartości (mniejszej niż 1) sprawia, że wysokie prawdopodobieństwa przyczyniają się bardziej. Gdy ustawiamy temperaturę na zero, wybieramy tylko najbardziej prawdopodobne wyjścia.\n",
        "\n",
        "Możemy zobaczyć te efekty poprzez dostosowanie argumentu `temperature`.\n"
      ]
    },
    {
      "metadata": {
        "id": "zbRXU5j1LR3_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def print_eval(e_syl):\n",
        "  display(HTML(format_html(fix_punctuation(decode_tokens(syl2str(e_syl, delim=''))))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4MTNidlS6j2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "prime_tok = str2syl2tok('Litwo! Ojczyzno moja!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBm5ibSnXM5p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_eval(evaluate(prime_tok, 200, temperature=0.8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DnStt-FzXM5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Niższe temperatury daja mniejszą różnorodność, wybierając tylko bardziej prawdopodobne wyjścia:"
      ]
    },
    {
      "metadata": {
        "id": "D3mkq_sCXM5r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_eval(evaluate(prime_tok, 200, temperature=0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l508QP1LXM5t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wyższe temperatury są bardziej różnorodne, wybierając mniej prawdopodobne wyjścia:"
      ]
    },
    {
      "metadata": {
        "id": "SxhfgI-PXM5u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_eval(evaluate(prime_tok, 200, temperature=1.4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuHoo0ARtjz_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!uptime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-UQoZYk2XM5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ćwiczenia\n",
        "\n",
        "* Trenuj z własnym zestawem danych, np.\n",
        "     * Tekst od innego autora\n",
        "     * Posty na blogu\n",
        "     * Kody źródłowe\n",
        "* Zwiększ liczbę warstw i rozmiar sieci, aby uzyskać lepsze wyniki"
      ]
    },
    {
      "metadata": {
        "id": "pFHHvkI6XM5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Następnie**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "2tJDT2lCceZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (debug) Monitorowanie maszyny wirtualnej"
      ]
    },
    {
      "metadata": {
        "id": "bKdCiolHcg4e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "def print_memsize():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f'{process.memory_info().rss / 1024**3:.5} GB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diBUUyOociRy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_memsize()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}