{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "syllable-rnn-generation-pl.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1DztHxtFb_tfNmrLeOcav9myCNqeTlZ82",
          "timestamp": 1524492157904
        },
        {
          "file_id": "1sv_cPvPEWEOrG9wS3fPjAOj-Wvs6kc0P",
          "timestamp": 1524467882593
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iVsYe7S5XM5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/eBRPvWB.png)\n",
        "\n",
        "# Generowanie poezji za pomocą RNN i PyTorch w oparciu o sylaby.\n",
        "\n",
        "[W tutorialu](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) użyliśmy RNN, aby sklasyfikować tekst po jednym znaku na raz. Tym razem wygenerujemy tekst po jedej sylabie na raz.\n",
        "```\n",
        "> python generate.py -n 500\n",
        "\n",
        "PAOLTREDN:\n",
        "Let, yil exter shis owrach we so sain, fleas,\n",
        "Be wast the shall deas, puty sonse my sheete.\n",
        "\n",
        "BAUFIO:\n",
        "Sirh carrow out with the knonuot my comest sifard queences\n",
        "O all a man unterd.\n",
        "\n",
        "PROMENSJO:\n",
        "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
        "An as shalp will of that seal think.\n",
        "\n",
        "NUKINUS:\n",
        "And house it to thee word off hee:\n",
        "And thou charrota the son hange of that shall denthand\n",
        "For the say hor you are of I folles muth me?\n",
        "```\n",
        "\n",
        "This one might make you question the series title — \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more.\n",
        "\n",
        "Zobacz [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) żeby nauczyć się więcej w tym temacie."
      ]
    },
    {
      "metadata": {
        "id": "WamMk47AXM5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Warto przeczytać\n",
        "\n",
        "Zakładam, że zainstalowałeś przynajmniej PyTorch, znasz Pythona i rozumiesz Tensory:\n",
        "\n",
        "* http://pytorch.org/ Instrukcje instalacji\n",
        "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) jak zacząć z PyTorch (ogólnie)\n",
        "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) dla szczegółowego przeglądu\n",
        "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) jeśli byłeś użytkownikiem Lua Torch\n",
        "\n",
        "Przydałoby się również wiedzieć o RNN'ach i ich działaniu:\n",
        "\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) pokazuje kilka przykładów z życia\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) dotyczy LSTM'ów, ale też podaje ogólne informacje na temat RNN\n",
        "\n",
        "Zobacz także podobne tutoriale z serii:\n",
        "\n",
        "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) używa RNN do klasyfikacji\n",
        "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) opierając się na tym modelu, dodaje kategorię jako dane wejściowe"
      ]
    },
    {
      "metadata": {
        "id": "g8KbaMvrXM5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie danych"
      ]
    },
    {
      "metadata": {
        "id": "9wuZu6mE057C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Plik wejściowy to duży plik tekstowy. Dzielimy go na sylaby."
      ]
    },
    {
      "metadata": {
        "id": "IKPynzs-yyW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Podzielenie korpusu na sylaby"
      ]
    },
    {
      "metadata": {
        "id": "80ZjgATAXsVn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "dataset_path = Path('data/rnn_generator'); dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'\n",
        "!mkdir -p $tmp_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPFXCZguyyXA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah $dataset_path/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eTRgNsMyyXD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
        "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efaGn-TLyyXG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "platform_suffixes = {'Linux': 'linux', 'Darwin': 'macos'}\n",
        "import platform\n",
        "platform_suffix = platform_suffixes[platform.system()]\n",
        "stemmer_bin = f'bin/stemmer.{platform_suffix}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3qYJ20EyyXI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# !$stemmer_bin -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGbQYAUYyyXK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!$stemmer_bin -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_char -o $fn_corpus_syl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpf6QibJyyXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Załadowanie danych"
      ]
    },
    {
      "metadata": {
        "id": "OSyGzgVD057R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ładujemy plik do pamięci i tokenizujemy. Tworzymy też listę wszystkich tokenów `all_tokens`."
      ]
    },
    {
      "metadata": {
        "id": "g9H83p3sXM5L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "file = open(fn_corpus_syl).read()\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maVrQMtZyyXU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# taken from fastai/text.py\n",
        "import re, string\n",
        "# remove +,- chars from punctuation set to keep syllables e.g.'--PO++' intact\n",
        "punctuation=re.sub('[\\+-]', '', string.punctuation)\n",
        "re_tok = re.compile(f'([{punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
        "\n",
        "file_tok = tokenize(file); len(file_tok), file_tok[:8]\n",
        "file_tok_len = len(file_tok)\n",
        "\n",
        "all_tokens = sorted(list(set(file_tok)))\n",
        "n_tokens = len(all_tokens); print(n_tokens, all_tokens[:50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lL_XOG-fXM5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aby stworzyć 'wejścia' z tego dużego ciągu danych, podzielimy go na kawałki."
      ]
    },
    {
      "metadata": {
        "id": "_zwmRSAHXM5T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 400\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_tok_len - chunk_len -1)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file_tok[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzAAi_95yyXq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def syl2str(a_list, delim='/'): return ' '.join(a_list).replace('++ --', delim)\n",
        "print(syl2str(random_chunk()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2cIY0S4g0gZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GPU"
      ]
    },
    {
      "metadata": {
        "id": "2hTjsshick9K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5cFLx6WXM5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Budowanie modelu\n",
        "\n",
        "Ten model przyjmie jako wejściie token dla kroku $ t _ {- 1} $ i ma wyprowadzić następny token $ t $. Istnieją trzy warstwy - jedna warstwa liniowa, która koduje znak wejściowy do stanu wewnętrznego, jedna warstwa GRU (która może sama mieć wiele warstw), która działa na tym stanie wewnętrznym i stanie ukrytym, oraz warstwa dekodera, która wyprowadza rozkład prawdopodobieństwa."
      ]
    },
    {
      "metadata": {
        "id": "tZ8chQcVXM5X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62XSRFgkXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensory wejściowe i docelowe"
      ]
    },
    {
      "metadata": {
        "id": "UaOPvn0rXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Każdy 'kawałek' zostanie przekształcony w tensor, a dokładnie w `LongTensor` (używany do wartości całkowitych), poprzez przepuszczenie wszystkich tokenów ciągu i wyszukiwanie indeksu każdej sylaby w `all_tokens`."
      ]
    },
    {
      "metadata": {
        "id": "q0H2nwMMXM5a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn token list into list of longs\n",
        "def tok_tensor(token_list):\n",
        "    tensor = torch.zeros(len(token_list)).long()\n",
        "    for c in range(len(token_list)):\n",
        "        tensor[c] = all_tokens.index(token_list[c])\n",
        "    return Variable(to_gpu(tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrTEI7EjyyX2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "a_token_list = file_tok[20:30]; print(a_token_list)\n",
        "print(tok_tensor(a_token_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Su49JvFXM5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wreszcie możemy zmontować parę tensorów wejściowych i docelowych do treningu, z losowego kawałka. Wejściem zostaną wszystkie tokeny * aż do przedostatniego*, a celem (targetem) będą wszystkie tokeny * od drugiego*. Jeśli więc nasz kawałek to \"abc\", wejście będzie odpowiadać \"ab\", podczas gdy cel to \"bc\"."
      ]
    },
    {
      "metadata": {
        "id": "mLzzsbTRXM5d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = tok_tensor(chunk[:-1])\n",
        "    target = tok_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSJ_szQTXM5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ewaluacja\n",
        "\n",
        "Aby ocenić sieć, będziemy podawać po jednym tokenie na raz, wykorzystywać wyjścia sieci jako rozkład prawdopodobieństwa dla następnego znaku i powtarzać. Aby rozpocząć generowanie, przekazujemy ciąg wstępny, aby rozpocząć budowanie stanu ukrytego, z którego następnie generujemy po jednym tokenie na raz."
      ]
    },
    {
      "metadata": {
        "id": "2ecqC4rWXM5f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_tokl=[all_tokens[1]], predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = tok_tensor(prime_tokl)\n",
        "    predicted = list(prime_tokl)  # need a copy of the list\n",
        "\n",
        "    # Use priming token list to \"build up\" hidden state\n",
        "    for p in range(len(prime_tokl) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        \n",
        "        # in pytorch 0.4.0 max, min fail if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/6996\n",
        "        # in all pytorch versions multinomial fails if there are Infs or nans\n",
        "        # https://github.com/pytorch/pytorch/issues/871\n",
        "        # temp fix, kill Infs and nans\n",
        "        # https://discuss.pytorch.org/t/how-to-set-inf-in-tensor-variable-to-0/10235\n",
        "        output_dist[output_dist == float(\"Inf\")] = 0\n",
        "        output_dist[output_dist == float(\"nan\")] = 0\n",
        "        \n",
        "        top_i = torch.multinomial(output_dist, 1)[0].item()\n",
        "        \n",
        "        # Add predicted token to the list and use as next input\n",
        "        predicted_token = all_tokens[top_i]\n",
        "        predicted.append(predicted_token)\n",
        "        inp = tok_tensor([predicted_token])\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZY0fPgEXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Trenowanie modelu"
      ]
    },
    {
      "metadata": {
        "id": "-pM5T97tXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Funkcja pomocnicza do wydrukowania upływającego czasu:"
      ]
    },
    {
      "metadata": {
        "id": "hQnLeX-TXM5h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0pntPTWEXM5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Główna funkcja treningowa"
      ]
    },
    {
      "metadata": {
        "id": "QKb7-MeXXM5j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].expand(1))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2EiZtEsA0571",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Opcjonalny monitoring postępu treningu"
      ]
    },
    {
      "metadata": {
        "id": "HpYR9j3qyyYF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "USE_VISDOM = False\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "vis = None\n",
        "if USE_VISDOM:\n",
        "    import visdom\n",
        "    vis = visdom.Visdom(port=8890)\n",
        "\n",
        "def vis_update_line_chart(vis, name, x, y, first_step):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.line(Y=np.array([y]), X=np.array([x]), win=name, opts=dict(title=name),\n",
        "             update=None if first_step else 'append')\n",
        "\n",
        "def vis_update_text_win(vis, name, text):\n",
        "    if not USE_VISDOM: return\n",
        "    vis.text(text, win=name, opts=dict(title=name), append=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFj5nGp5yyYI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class X(str):\n",
        "    def rpl(self, p, c='lightgray'):\n",
        "        return X(self.replace(p, f'<font color=\"{c}\">{p}</font>'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tY2U1vt4yyYK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def bad_words(e_syl): e_str = syl2str(e_syl); return (e_str.count('++') + e_str.count('--')) / len(e_syl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyo69fakXM5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Następnie definiujemy parametry treningowe i rozpoczynamy trening:"
      ]
    },
    {
      "metadata": {
        "id": "ILThkaRPXM5k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 3000   # 2000\n",
        "print_every = 100 # 100\n",
        "plot_every = 50\n",
        "hidden_size = 500 # 100\n",
        "n_layers = 3 # 1\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_tokens, hidden_size, n_tokens, n_layers)\n",
        "if USE_GPU:\n",
        "    decoder.cuda()\n",
        "print(decoder, flush=True)\n",
        "\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if USE_GPU:\n",
        "    criterion.cuda()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "all_bw = []\n",
        "bw_avg = 0\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "iterable = range(1, n_epochs + 1)\n",
        "tqdm_ = tqdm(iterable, '', leave=False, dynamic_ncols=True)\n",
        "first_step = True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6y2kdZUj2bw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in tqdm_:\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    # current loss chart\n",
        "    vis_update_line_chart(vis, 'loss', epoch, loss, epoch == 1)\n",
        "    \n",
        "    # progress_bar\n",
        "    tqdm_.set_postfix({'loss': loss})\n",
        "    text = f'&nbsp;<font color=\"red\">{tqdm_}</font>'\n",
        "    vis_update_text_win(vis, 'progress_bar', text)\n",
        "\n",
        "    # bad words\n",
        "    prime_str = file_tok[13:18]\n",
        "    bw = bad_words(evaluate(prime_str, 100))\n",
        "    bw_avg += bw\n",
        "\n",
        "    # current bad words chart\n",
        "    vis_update_line_chart(vis, 'bad_words', epoch, bw, epoch == 1)\n",
        "    \n",
        "    if epoch % print_every == 0:\n",
        "        e_syl = evaluate(prime_str, 100)\n",
        "        e_str = syl2str(e_syl)\n",
        "        stats_str = '\\n[%s (%d %d%%) loss=%.4f bw=%.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, bw)\n",
        "        print(stats_str)\n",
        "        e_html = X(e_str).rpl('/').rpl('--', c='red').rpl('++', c='red')\n",
        "        print(e_str, '\\n', flush=True)\n",
        "        text = f'<b>{stats_str}</b><br />{e_html}'\n",
        "        vis_update_text_win(vis, 'evaluation', text)\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        vis_update_line_chart(vis, 'loss_avg', epoch, loss_avg / plot_every, first_step)\n",
        "        vis_update_line_chart(vis, 'bad_words_avg', epoch, bw_avg / plot_every, first_step)\n",
        "        all_bw.append(bw)\n",
        "        bw_avg = 0\n",
        "        first_step = False\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXrPD94SXM5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Rysowanie wykresu funkcji straty\n",
        "\n",
        "Wykreślanie historycznej straty z all_losses pokazuje uczenie sieci:"
      ]
    },
    {
      "metadata": {
        "id": "meKCxPo3XM5n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.style.use('default')\n",
        "mpl.style.use('bmh')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EtK-9BuuB-GX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Zapisanie modelu"
      ]
    },
    {
      "metadata": {
        "id": "aSA3Utpc6dXV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ALLTOKS, MODEL = ['all_tokens', 'model']\n",
        "\n",
        "fn_pan_tadeusz = {ALLTOKS: 'all_tokens.pan_tadeusz.p', MODEL: 'pan_tadeusz.h300.l2.e3000.gpu.torch'}\n",
        "\n",
        "fn_dict = fn_pan_tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ-j5_4I6d0g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save all_tokens\n",
        "import pickle\n",
        "\n",
        "pickle.dump(all_tokens, open(tmp_path / fn_dict[ALLTOKS], 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDm3bjJy6d3S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# all_tokens = pickle.load( open( tmp_path / fn_dict[ALLTOKS], 'rb' ) )\n",
        "# n_characters = len(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHbGgVULB_cp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "decoder.state_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRXLplmTCumA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# save model\n",
        "model_path = tmp_path / fn_dict[MODEL]\n",
        "torch.save(decoder, model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-D9_ONXNMHPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        " # decoder = torch.load(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tm9jbTN6Wfl-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dt8_7GshWi4M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah $tmp_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fEFWi-FXM5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ewaluacja w różnych \"temperaturach\"\n",
        "\n",
        "W powyższej funkcji `evaluate`, za każdym razem, gdy dokonywana jest prognoza, wyjścia są dzielone przez przekazany argument \"temperature\". Użycie większej liczby sprawia, że wszystkie akcje są bardziej jednakowo prawdopodobne, a tym samym dają nam \"bardziej losowe\" wyniki. Użycie mniejszej wartości (mniejszej niż 1) sprawia, że wysokie prawdopodobieństwa przyczyniają się bardziej. Gdy ustawiamy temperaturę na zero, wybieramy tylko najbardziej prawdopodobne wyjścia.\n",
        "\n",
        "Możemy zobaczyć te efekty poprzez dostosowanie argumentu `temperature`.\n"
      ]
    },
    {
      "metadata": {
        "id": "gBm5ibSnXM5p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "prime_tokl = file_tok[13:18]\n",
        "print(syl2str(evaluate(prime_tokl, 200, temperature=0.8), delim='/'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DnStt-FzXM5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Niższe temperatury daja mniejszą różnorodność, wybierając tylko bardziej prawdopodobne wyjścia:"
      ]
    },
    {
      "metadata": {
        "id": "D3mkq_sCXM5r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(syl2str(evaluate(prime_tokl, 200, temperature=0.2), delim='/'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l508QP1LXM5t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wyższe temperatury są bardziej różnorodne, wybierając mniej prawdopodobne wyjścia:"
      ]
    },
    {
      "metadata": {
        "id": "SxhfgI-PXM5u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(syl2str(evaluate(prime_tokl, 200, temperature=1.4), delim='/'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tJDT2lCceZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Monitorowanie maszyny wirtualnej"
      ]
    },
    {
      "metadata": {
        "id": "bKdCiolHcg4e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "def print_memsize():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f'{process.memory_info().rss / 1024**3:.5} GB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diBUUyOociRy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_memsize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuHoo0ARtjz_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!uptime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFJdzy4PTnsO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# who"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-UQoZYk2XM5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ćwiczenia\n",
        "\n",
        "* Trenuj z własnym zestawem danych, np.\n",
        "     * Tekst od innego autora\n",
        "     * Posty na blogu\n",
        "     * Kody źródłowe\n",
        "* Zwiększ liczbę warstw i rozmiar sieci, aby uzyskać lepsze wyniki"
      ]
    },
    {
      "metadata": {
        "id": "pFHHvkI6XM5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Następnie**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
      ]
    }
  ]
}