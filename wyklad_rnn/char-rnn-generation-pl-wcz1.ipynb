{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVsYe7S5XM5G"
   },
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Generowanie sztuki za pomocą RNN i PyTorch w oparciu o litery.\n",
    "\n",
    "[W tutorialu](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) użyliśmy RNN, aby sklasyfikować tekst po jednym znaku na raz. Tym razem wygenerujemy tekst po jednym znaku na raz.\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "Zobacz [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) żeby nauczyć się więcej w tym temacie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WamMk47AXM5I"
   },
   "source": [
    "## Warto przeczytać\n",
    "\n",
    "Zakładam, że zainstalowałeś przynajmniej PyTorch, znasz Pythona i rozumiesz Tensory:\n",
    "\n",
    "* http://pytorch.org/ Instrukcje instalacji\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) na początek z PyTorch (ogólnie)\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) dla szczegółowego przeglądu\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) jeśli jesteś poprzednim użytkownikiem Lua Torch\n",
    "\n",
    "Przydałoby się również wiedzieć o RNN i jego działaniu:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) pokazuje kilka przykładów z życia\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) dotyczy tylko LSTM, ale także informacji na temat RNN w ogóle\n",
    "\n",
    "Zobacz także podobne tutoriale z serii:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) używa RNN do klasyfikacji\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) opiera się na tym modelu, aby dodać kategorię jako dane wejściowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8KbaMvrXM5J"
   },
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podzielenie korpusu na sylaby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "80ZjgATAXsVn"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_path = Path('data/rnn_generator'); dataset_path\n",
    "tmp_path = dataset_path / 'tmp/'\n",
    "!mkdir -p $tmp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah $dataset_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_corpus_char = dataset_path/'pan_tadeusz.txt'\n",
    "fn_corpus_syl = dataset_path/'pan_tadeusz.syl1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_suffixes = {'Linux': 'linux', 'Darwin': 'macos'}\n",
    "import platform\n",
    "platform_suffix = platform_suffixes[platform.system()]\n",
    "stemmer_bin = f'bin/stemmer.{platform_suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !$stemmer_bin -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$stemmer_bin -s 7683 -v -d bin/stemmer2.dic -i $fn_corpus_char -o $fn_corpus_syl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Załadowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8KbaMvrXM5J"
   },
   "source": [
    "Plik, którego używamy, jest plikiem tekstowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YLH7c2gwoLlP"
   },
   "outputs": [],
   "source": [
    "# fn = 'data/tiny-shakespeare.txt'\n",
    "# fn = dataset_path / 'mickiewicz.txt'\n",
    "# fn = dataset_path / 'witkacy_szewcy.txt'\n",
    "fn = dataset_path / 'pan_tadeusz.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1525,
     "status": "ok",
     "timestamp": 1525785563672,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "g9H83p3sXM5L",
    "outputId": "d45dd41b-b684-48dd-bce2-034533b9e236"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "file = open(fn).read()\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# all chars found in file\n",
    "all_characters = sorted(list(set(file))); print(all_characters[:10])\n",
    "n_characters = len(all_characters); print(n_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL_XOG-fXM5T"
   },
   "source": [
    "Aby wprowadzić dane z tego dużego ciągu danych, podzielimy je na fragmenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1525785564316,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "_zwmRSAHXM5T",
    "outputId": "0806d1df-c428-4561-8b9e-10a39312e3ea"
   },
   "outputs": [],
   "source": [
    "chunk_len = 400\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5cFLx6WXM5X"
   },
   "source": [
    "## Budowanie modelu\n",
    "\n",
    "Ten model przyjmie jako znak wejściowy znak dla kroku $ t _ {- 1} $ i ma wyprowadzić następny znak $ t $. Istnieją trzy warstwy - jedna warstwa liniowa, która koduje znak wejściowy do stanu wewnętrznego, jedna warstwa GRU (która może sama mieć wiele warstw), która działa na tym stanie wewnętrznym i stanie ukrytym, oraz warstwa dekodera, która wyprowadza rozkład prawdopodobieństwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cIY0S4g0gZo"
   },
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1490,
     "status": "ok",
     "timestamp": 1525785560764,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "2hTjsshick9K",
    "outputId": "3e44ab87-6844-4a1c-d4fe-42c135ddda87"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = torch.cuda.is_available(); \n",
    "# USE_GPU = False; \n",
    "\n",
    "print(f'USE_GPU={USE_GPU}')\n",
    "\n",
    "def to_gpu(x, *args, **kwargs):\n",
    "    return x.cuda(*args, **kwargs) if USE_GPU else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tZ8chQcVXM5X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62XSRFgkXM5Z"
   },
   "source": [
    "## Wejścia i Wyjścia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaOPvn0rXM5Z"
   },
   "source": [
    "Każdy fragment zostanie przekształcony w tensor, w szczególności \"LongTensor\" (używany do wartości całkowitych), poprzez zapętlenie znaków ciągu i wyszukiwanie indeksu każdego znaku w `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3296,
     "status": "ok",
     "timestamp": 1525785569976,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "q0H2nwMMXM5a",
    "outputId": "8aba671f-aa8f-4d08-f337-8c85d5bd64b6"
   },
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(to_gpu(tensor))\n",
    "\n",
    "print(char_tensor('ala ma kota'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Su49JvFXM5d"
   },
   "source": [
    "Wreszcie możemy zebrać parę tensorów wejściowych i docelowych do treningu, z losowego kawałka. Wprowadzone zostaną wszystkie znaki * aż do ostatniej *, a celem będą wszystkie znaki * od pierwszego *. Jeśli więc nasz fragment to \"abc\", dane wejściowe będą odpowiadać \"ab\", podczas gdy cel to \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mLzzsbTRXM5d"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSJ_szQTXM5f"
   },
   "source": [
    "## Ewaluacja\n",
    "\n",
    "Aby ocenić sieć, będziemy podawać po jednym znaku na raz, wykorzystywać wyjścia sieci jako rozkład prawdopodobieństwa dla następnego znaku i powtarzać. Aby rozpocząć generowanie, przekazujemy ciąg wstępny, aby rozpocząć budowanie stanu ukrytego, z którego następnie generujemy po jednym znaku na raz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2ecqC4rWXM5f"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZY0fPgEXM5h"
   },
   "source": [
    "## Trenowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pM5T97tXM5h"
   },
   "source": [
    "Pomocnik do wydrukowania upływającego czasu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hQnLeX-TXM5h"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pntPTWEXM5i"
   },
   "source": [
    "Główna funkcja treningowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QKb7-MeXXM5j"
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].expand(1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyo69fakXM5k"
   },
   "source": [
    "Następnie definiujemy parametry treningowe i rozpoczynamy trening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4219
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9739571,
     "status": "ok",
     "timestamp": 1525795313357,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "ILThkaRPXM5k",
    "outputId": "1ee80d09-abf0-4bd4-d0bc-0638ff180d98"
   },
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "n_epochs = 3000   # 2000\n",
    "print_every = 100 # 100\n",
    "plot_every = 1\n",
    "hidden_size = 300 # 100\n",
    "n_layers = 4 # 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "if USE_GPU:\n",
    "  decoder.cuda()\n",
    "print(decoder, flush=True)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if USE_GPU:\n",
    "  criterion.cuda()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        e = evaluate('Wh', 200)\n",
    "        print('\\n[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(e, '\\n', flush=True)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXrPD94SXM5m"
   },
   "source": [
    "## Rysowanie wykresu funkcji straty\n",
    "\n",
    "Wykreślanie historycznej straty z all_losses pokazuje uczenie sieci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1525795314903,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "meKCxPo3XM5n",
    "outputId": "40d7a20b-8f87-4d33-d6a2-5eb7ea7478dc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fEFWi-FXM5p"
   },
   "source": [
    "## Ocena w różnych \"temperaturach\"\n",
    "\n",
    "W powyższej funkcji `evaluate`, za każdym razem, gdy dokonywana jest prognoza, wyjścia są dzielone przez przekazany argument \"temperature\". Używanie większej liczby sprawia, że wszystkie akcje są bardziej jednakowo prawdopodobne, a tym samym dają nam \"bardziej losowe\" (\"more random\") wyniki. Używanie mniejszej wartości (mniejszej niż 1) sprawia, że wysokie prawdopodobieństwa przyczyniają się bardziej. Gdy ustawiamy temperaturę na zero, wybieramy tylko najbardziej prawdopodobne wyjścia.\n",
    "\n",
    "Możemy to zobaczyć poprzez dostosowanie argumentu `temperature`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1525795318273,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "gBm5ibSnXM5p",
    "outputId": "791d8e4d-9ef6-4b36-a2e0-9749d53713a0"
   },
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnStt-FzXM5r"
   },
   "source": [
    "[link text](https://)Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1525795319265,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "D3mkq_sCXM5r",
    "outputId": "c574d9a0-173e-4089-b5be-a8c7fbbaf472"
   },
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l508QP1LXM5t"
   },
   "source": [
    "Higher temperatures more varied, choosing less probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1525795320268,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "SxhfgI-PXM5u",
    "outputId": "4c55b602-a5ac-43cc-f4b9-3271296c4356"
   },
   "outputs": [],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UQoZYk2XM5v"
   },
   "source": [
    "## Ćwiczenia\n",
    "\n",
    "* Trenuj z własnym zestawem danych, np.\n",
    "     * Tekst od innego autora\n",
    "     * Posty na blogu\n",
    "     * Kod\n",
    "* Zwiększ liczbę warstw i rozmiar sieci, aby uzyskać lepsze wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFHHvkI6XM5w"
   },
   "source": [
    "**Następnie**: [Generowanie nazw z warunkowym poziomem RNN na poziomie znaku](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtK-9BuuB-GX"
   },
   "source": [
    "## Zapisanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aSA3Utpc6dXV"
   },
   "outputs": [],
   "source": [
    "ALLCHARS, MODEL = ['all_characters', 'model']\n",
    "\n",
    "fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h400.l3.3000.gpu.torch'}\n",
    "\n",
    "fn_dict = fn_pan_tadeusz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XJ-j5_4I6d0g"
   },
   "outputs": [],
   "source": [
    "# save all_characters\n",
    "import pickle\n",
    "\n",
    "pickle.dump(all_characters, open(tmp_path / fn_dict[ALLCHARS], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CDm3bjJy6d3S"
   },
   "outputs": [],
   "source": [
    "# all_characters = pickle.load( open( tmp_path / fn_dict[ALLCHARS], 'rb' ) )\n",
    "# n_characters = len(all_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwOSEhKulCwX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1525795323213,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "qHbGgVULB_cp",
    "outputId": "4297fe1b-0f18-40d0-f7d6-ba25043080b6"
   },
   "outputs": [],
   "source": [
    "decoder.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1525795324005,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "nRXLplmTCumA",
    "outputId": "14d3aa5f-1736-479c-f32e-720a1594dee2"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_path = tmp_path / fn_dict[MODEL]\n",
    "torch.save(decoder, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-D9_ONXNMHPK"
   },
   "outputs": [],
   "source": [
    " # decoder = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1525795325642,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "Tm9jbTN6Wfl-",
    "outputId": "6d4d5a41-7acf-4825-d596-23e357d598ae"
   },
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1839,
     "status": "ok",
     "timestamp": 1525795327519,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "Dt8_7GshWi4M",
    "outputId": "8b0b1926-ca6b-499c-89ca-400c249fb53b"
   },
   "outputs": [],
   "source": [
    "ls -lah /content/data/rnn_generator/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tJDT2lCceZL"
   },
   "source": [
    "## Monitorowanie maszyny wirtualnej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bKdCiolHcg4e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def print_memsize():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(f'{process.memory_info().rss / 1024**3:.5} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1525785557493,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "diBUUyOociRy",
    "outputId": "6a8644cb-d9ca-4d8f-dab5-74e5bf35633a"
   },
   "outputs": [],
   "source": [
    "print_memsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1525785559261,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "TuHoo0ARtjz_",
    "outputId": "962d493e-faf8-46d6-919e-3835d02857c4"
   },
   "outputs": [],
   "source": [
    "!uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1525795317277,
     "user": {
      "displayName": "Kamsiulek Malutki",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118138569128465864586"
     },
     "user_tz": -120
    },
    "id": "bFJdzy4PTnsO",
    "outputId": "867e92af-333e-4a8f-c49d-503b01f92c82"
   },
   "outputs": [],
   "source": [
    "who"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "char-rnn-generation-wcz3-gpu.ipynb",
   "provenance": [
    {
     "file_id": "1DztHxtFb_tfNmrLeOcav9myCNqeTlZ82",
     "timestamp": 1524492157904
    },
    {
     "file_id": "1sv_cPvPEWEOrG9wS3fPjAOj-Wvs6kc0P",
     "timestamp": 1524467882593
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
