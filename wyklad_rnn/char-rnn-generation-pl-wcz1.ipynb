{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char-rnn-generation-wcz3-gpu.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1DztHxtFb_tfNmrLeOcav9myCNqeTlZ82",
          "timestamp": 1524492157904
        },
        {
          "file_id": "1sv_cPvPEWEOrG9wS3fPjAOj-Wvs6kc0P",
          "timestamp": 1524467882593
        }
      ],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iVsYe7S5XM5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/eBRPvWB.png)\n",
        "\n",
        "# Generowanie sztuki za pomocą RNN i PyTorch w oparciu o litery.\n",
        "\n",
        "[W tutorialu](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) użyliśmy RNN, aby sklasyfikować tekst po jednym znaku na raz. Tym razem wygenerujemy tekst po jednym znaku na raz.\n",
        "```\n",
        "> python generate.py -n 500\n",
        "\n",
        "PAOLTREDN:\n",
        "Let, yil exter shis owrach we so sain, fleas,\n",
        "Be wast the shall deas, puty sonse my sheete.\n",
        "\n",
        "BAUFIO:\n",
        "Sirh carrow out with the knonuot my comest sifard queences\n",
        "O all a man unterd.\n",
        "\n",
        "PROMENSJO:\n",
        "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
        "An as shalp will of that seal think.\n",
        "\n",
        "NUKINUS:\n",
        "And house it to thee word off hee:\n",
        "And thou charrota the son hange of that shall denthand\n",
        "For the say hor you are of I folles muth me?\n",
        "```\n",
        "\n",
        "Zobacz [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) żeby nauczyć się więcej w tym temacie."
      ]
    },
    {
      "metadata": {
        "id": "WamMk47AXM5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Warto przeczytać\n",
        "\n",
        "Zakładam, że zainstalowałeś przynajmniej PyTorch, znasz Pythona i rozumiesz Tensory:\n",
        "\n",
        "* http://pytorch.org/ Instrukcje instalacji\n",
        "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) na początek z PyTorch (ogólnie)\n",
        "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) dla szczegółowego przeglądu\n",
        "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) jeśli jesteś poprzednim użytkownikiem Lua Torch\n",
        "\n",
        "Przydałoby się również wiedzieć o RNN i jego działaniu:\n",
        "\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) pokazuje kilka przykładów z życia\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) dotyczy tylko LSTM, ale także informacji na temat RNN w ogóle\n",
        "\n",
        "Zobacz także podobne tutoriale z serii:\n",
        "\n",
        "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) używa RNN do klasyfikacji\n",
        "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) opiera się na tym modelu, aby dodać kategorię jako dane wejściowe"
      ]
    },
    {
      "metadata": {
        "id": "lGN3xU17XpWC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Instalacja"
      ]
    },
    {
      "metadata": {
        "id": "kkkCqSEBXsYh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d15f6520-5024-4f2d-93b8-51a50662a16b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785552435,
          "user_tz": -120,
          "elapsed": 1716,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "80ZjgATAXsVn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "dataset_path = Path.home() / 'data/rnn_generator'; dataset_path\n",
        "tmp_path = dataset_path / 'tmp/'\n",
        "!mkdir -p $tmp_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4wAU8Y6XsSl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "315cd5c2-8005-4e9d-9d9e-91ff750d2a63",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785556030,
          "user_tz": -120,
          "elapsed": 1718,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lah /content/data/rnn_generator"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.1M\r\n",
            "drwxr-xr-x 3 root root 4.0K May  8 13:19 .\r\n",
            "drwxr-xr-x 3 root root 4.0K May  8 13:10 ..\r\n",
            "-rw-r--r-- 1 root root 709K May  8 13:11 mickiewicz.txt\r\n",
            "-rw-r--r-- 1 root root 219K May  8 13:11 pan_tadeusz.txt\r\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:19 tmp\r\n",
            "-rw-r--r-- 1 root root 142K May  8 13:10 witkacy_szewcy.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2tJDT2lCceZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Monitorowanie wirtualnej maszyny"
      ]
    },
    {
      "metadata": {
        "id": "bKdCiolHcg4e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "def print_memsize():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f'{process.memory_info().rss / 1024**3:.5} GB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diBUUyOociRy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a8644cb-d9ca-4d8f-dab5-74e5bf35633a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785557493,
          "user_tz": -120,
          "elapsed": 694,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print_memsize()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.13219 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TuHoo0ARtjz_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "962d493e-faf8-46d6-919e-3835d02857c4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785559261,
          "user_tz": -120,
          "elapsed": 1728,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!uptime"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 13:19:18 up 16 min,  0 users,  load average: 0.08, 0.22, 0.38\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2cIY0S4g0gZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ]
    },
    {
      "metadata": {
        "id": "2hTjsshick9K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e44ab87-6844-4a1c-d4fe-42c135ddda87",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785560764,
          "user_tz": -120,
          "elapsed": 1490,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "USE_GPU = torch.cuda.is_available(); \n",
        "# USE_GPU = False; \n",
        "\n",
        "print(f'USE_GPU={USE_GPU}')\n",
        "\n",
        "def to_gpu(x, *args, **kwargs):\n",
        "    return x.cuda(*args, **kwargs) if USE_GPU else x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE_GPU=True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g8KbaMvrXM5J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Przygotowanie danych\n",
        "\n",
        "Plik, którego używamy, jest plikiem tekstowym. Przekształcamy wszelkie potencjalne znaki Unicode w zwykły ASCII, używając pakietu `unidecode` (który możesz zainstalować przez` pip` lub `conda`)."
      ]
    },
    {
      "metadata": {
        "id": "YLH7c2gwoLlP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# fn = 'data/tiny-shakespeare.txt'\n",
        "# fn = dataset_path / 'mickiewicz.txt'\n",
        "# fn = dataset_path / 'witkacy_szewcy.txt'\n",
        "fn = dataset_path / 'pan_tadeusz.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9H83p3sXM5L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d45dd41b-b684-48dd-bce2-034533b9e236",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785563672,
          "user_tz": -120,
          "elapsed": 1525,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "# file = unidecode.unidecode(open(fn).read())\n",
        "file = open(fn).read()\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "# ascii only\n",
        "# all_characters = string.printable\n",
        "# n_characters = len(all_characters)\n",
        "\n",
        "\n",
        "# all chars found in file\n",
        "all_characters = sorted(list(set(file))); print(all_characters[:10])\n",
        "n_characters = len(all_characters); print(n_characters)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 203037\n",
            "['\\n', ' ', '!', '\"', '%', \"'\", '(', ')', ',', '-']\n",
            "89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lL_XOG-fXM5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aby wprowadzić dane z tego dużego ciągu danych, podzielimy je na fragmenty."
      ]
    },
    {
      "metadata": {
        "id": "_zwmRSAHXM5T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0806d1df-c428-4561-8b9e-10a39312e3ea",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785564316,
          "user_tz": -120,
          "elapsed": 561,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 400\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "m w czoło stukał,\n",
            "Niby do wspomnień dawnych uśpionych w niém pukał,\n",
            "Nakoniec gryząc palce do krwi się zadrasnął,\n",
            "I na cały głos -- dobrze, dobrze mi tak! -- wrzasnął.\n",
            "\n",
            "We dworze, gdzie przed chwilą tyle było krzyku,\n",
            "Teraz pusto i głucho, jak na mogilniku:\n",
            "Wszyscy ruszyli w pole; Tadeusz nadstawił\n",
            "Uszu, i ręce do nich jak trąbki przyprawił,\n",
            "Słuchał, aż mu wiatr przyniósł wiejący od puszczy,\n",
            "Odgłosy \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5cFLx6WXM5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Budowanie modelu\n",
        "\n",
        "Ten model przyjmie jako znak wejściowy znak dla kroku $ t _ {- 1} $ i ma wyprowadzić następny znak $ t $. Istnieją trzy warstwy - jedna warstwa liniowa, która koduje znak wejściowy do stanu wewnętrznego, jedna warstwa GRU (która może sama mieć wiele warstw), która działa na tym stanie wewnętrznym i stanie ukrytym, oraz warstwa dekodera, która wyprowadza rozkład prawdopodobieństwa."
      ]
    },
    {
      "metadata": {
        "id": "tZ8chQcVXM5X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(to_gpu(torch.zeros(self.n_layers, 1, self.hidden_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62XSRFgkXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wejście/Wyjście"
      ]
    },
    {
      "metadata": {
        "id": "UaOPvn0rXM5Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Każdy fragment zostanie przekształcony w tensor, w szczególności \"LongTensor\" (używany do wartości całkowitych), poprzez zapętlenie znaków ciągu i wyszukiwanie indeksu każdego znaku w `all_characters`."
      ]
    },
    {
      "metadata": {
        "id": "q0H2nwMMXM5a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8aba671f-aa8f-4d08-f337-8c85d5bd64b6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525785569976,
          "user_tz": -120,
          "elapsed": 3296,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(to_gpu(tensor))\n",
        "\n",
        "print(char_tensor('ala ma kota'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            " 41\n",
            " 52\n",
            " 41\n",
            "  1\n",
            " 53\n",
            " 41\n",
            "  1\n",
            " 51\n",
            " 55\n",
            " 59\n",
            " 41\n",
            "[torch.cuda.LongTensor of size 11 (GPU 0)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Su49JvFXM5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wreszcie możemy zebrać parę tensorów wejściowych i docelowych do treningu, z losowego kawałka. Wprowadzone zostaną wszystkie znaki * aż do ostatniej *, a celem będą wszystkie znaki * od pierwszego *. Jeśli więc nasz fragment to \"abc\", dane wejściowe będą odpowiadać \"ab\", podczas gdy cel to \"bc\"."
      ]
    },
    {
      "metadata": {
        "id": "mLzzsbTRXM5d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSJ_szQTXM5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ewaluacja\n",
        "\n",
        "Aby ocenić sieć, będziemy podawać po jednym znaku na raz, wykorzystywać wyjścia sieci jako rozkład prawdopodobieństwa dla następnego znaku i powtarzać. Aby rozpocząć generowanie, przekazujemy ciąg wstępny, aby rozpocząć budowanie stanu ukrytego, z którego następnie generujemy po jednym znaku na raz."
      ]
    },
    {
      "metadata": {
        "id": "2ecqC4rWXM5f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZY0fPgEXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trenowanie modelu"
      ]
    },
    {
      "metadata": {
        "id": "-pM5T97tXM5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pomocnik do wydrukowania upływającego czasu:"
      ]
    },
    {
      "metadata": {
        "id": "hQnLeX-TXM5h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0pntPTWEXM5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Główna funkcja treningowa"
      ]
    },
    {
      "metadata": {
        "id": "QKb7-MeXXM5j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c])\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data[0] / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyo69fakXM5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Następnie definiujemy parametry treningowe i rozpoczynamy trening:"
      ]
    },
    {
      "metadata": {
        "id": "ILThkaRPXM5k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4219
        },
        "outputId": "1ee80d09-abf0-4bd4-d0bc-0638ff180d98",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795313357,
          "user_tz": -120,
          "elapsed": 9739571,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "n_epochs = 3000   # 2000\n",
        "print_every = 100 # 100\n",
        "plot_every = 1\n",
        "hidden_size = 300 # 100\n",
        "n_layers = 4 # 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "if USE_GPU:\n",
        "  decoder.cuda()\n",
        "print(decoder, flush=True)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if USE_GPU:\n",
        "  criterion.cuda()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(1, n_epochs + 1)):\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        e = evaluate('Wh', 200)\n",
        "        print('\\n[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print(e, '\\n', flush=True)\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (encoder): Embedding(89, 300)\n",
            "  (gru): GRU(300, 300, num_layers=4)\n",
            "  (decoder): Linear(in_features=300, out_features=89)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 3/3000 [00:09<2:40:20,  3.21s/it]/usr/local/lib/python3.6/dist-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
            "  TqdmSynchronisationWarning)\n",
            "  3%|▎         | 99/3000 [05:15<2:34:13,  3.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[5m 19s (100 3%) 3.1858]\n",
            "Wha c \n",
            "  za ha scsczo  \n",
            "sjłazieł c,zł sm wię łry ieswe dyc\n",
            "siytiizaswniacbęcw \n",
            "waietńs i kokaśtie\n",
            "\n",
            "tacs dlob   sba kPlwi rawwu  c\n",
            "cWki kę śz m iółes \n",
            "kę zek wietk wne Żak  ny ły\n",
            "kań \n",
            "s c ziewo sczii one \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 199/3000 [10:40<2:30:20,  3.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[10m 44s (200 6%) 3.1964]\n",
            "Whace p tkągedadzta  niwwiiriłediokpecąłiył cpocnz  Trczeda Padba iycocaśm mzoma,\n",
            "s  śsliececz, ciizeł kne t wanołzod  kone  pe o-niacs  ap c łopiihy zzemle wa tatrecmiedkie\n",
            "\n",
            "Izopajiawśiia nycnih s:\n",
            "\n",
            " \n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|▉         | 299/3000 [16:06<2:25:30,  3.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[16m 10s (300 10%) 2.9609]\n",
            "Whazylama żina wy pierłastnielłęw  wie\n",
            "A  niałełłkna,\n",
            "jokie kadoła pamnaświaśmiy,\n",
            "Pe,\n",
            "Dokkie wer szłaswie,\n",
            "Owasoma ramlie,\n",
            " ny płakła cietkac nion unie kut ka zebarna pawaszałaczew żzeedbeł w kałiyła na \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 399/3000 [21:31<2:20:17,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[21m 34s (400 13%) 2.8192]\n",
            "Whoceci banadtówl jzana piaśnék zadelowiem damierak mat szó gogaj, zawe wial niela dielac camagiaja i żzały dułotatie wzebam szeji sze i zenie\n",
            "Pacze jeno Sagaśn gum wora mogana wecowe tiagala kielometa  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 499/3000 [26:55<2:14:55,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[26m 58s (500 16%) 2.6752]\n",
            "Whama  i ponioczyrza azymąje w nacz ny ny,\n",
            "Czaniepu zajokyjdowa kana\n",
            "A\n",
            "Wtrrepawa.\n",
            "\n",
            "Senałomi\n",
            "Aciem jasi szasti sdaszę pidajia szsz rzeody siaszna szejy zu kozywi siupie dzazwiemu jasza gaie nieł waszy ji \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 599/3000 [32:20<2:09:37,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[32m 24s (600 20%) 2.6587]\n",
            "Whako\n",
            "I ma dzi;\n",
            "O tad ię łiwrzamiężsaść\n",
            "Tieke ssza ker mięta dała\n",
            "Zed madaciem pali machstę rzienowia szsu zszarna pra Byniał ta kosny Seta tozta ożaniewi brzeta wana pyjęsiana, więwo w snieny radłimy;\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 699/3000 [37:46<2:04:20,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[37m 50s (700 23%) 2.5110]\n",
            "Whrzycym i posszeki u swanisniszedzostom Totieszem skorze, poliestał.\n",
            "\n",
            "biewierze zkorze szesnąsnie szerzązsza stró szesadź\n",
            "Zké zaporzech zew liz to\n",
            "Zesiezstą przęzawe szyrzą wewe zieskołą sto pótea ząsi \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 799/3000 [43:09<1:58:52,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[43m 12s (800 26%) 2.4660]\n",
            "Wharaznt i tięm i dzasi weta zaly uszedziazą,\n",
            "Krzyny ożadtązim growie zdoda,\n",
            "Jędać żddziczema oie wdrzega gołejstach zie ndym tuwa stytrza julo wczewiagku, co go w łdorasi,\n",
            "W i larzał done wzielą goi zl \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|██▉       | 899/3000 [48:34<1:53:32,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[48m 38s (900 30%) 2.5266]\n",
            "Whorz kęrómiem borz nicha wkodod miy chodórw ojycho krzymty dorze,\n",
            "I wił dorzego miałonie rwe w kryne irzy trosi zugy i zcam, udzierzy wiył z kogo dydzym ddawydawie tywał «Plituł z grłyniy stogo o korłi \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 999/3000 [53:57<1:48:03,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[54m 0s (1000 33%) 2.6412]\n",
            "Whach siej wajał\n",
            "Zawieł, cieśł prelły padna wdot biecz mowady zdazewy ządy chchesch, chka! na jcoga umiego spu chuszy, Mego chzed drobych w, wczalą  ciac spoczysze, wach tara mie zsach ciewasze wiedzą c \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 1099/3000 [59:22<1:42:42,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[59m 26s (1100 36%) 2.5228]\n",
            "Whspów falsz piędocze czacie,\n",
            "Sodźma cicy!\n",
            "Rzeł kol polajczytęjze.\n",
            "Sar tęskie pie cacy przekł, postam obłowi krzę żąchi,\n",
            "Saś, ni ronie słochany, draw się banów, podki w wki.\n",
            "\n",
            " Biedzie pramaleńczy postęc \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 1199/3000 [1:04:44<1:37:14,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[64m 48s (1200 40%) 2.3097]\n",
            "Whod złoczajał stalrognać zamadna ni kranów szych sty staku, bał  jyd sapana to.\n",
            "Jawan Zeczm w oży jor u stadzu się skrawe,\n",
            "Nycia przadzniach bamał!\n",
            "Przysz zanu pustarzy many abnia podzani przat, z prab \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 1299/3000 [1:10:09<1:31:52,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[70m 13s (1300 43%) 2.5610]\n",
            "Whasczni bam głaniej drowy Supilszy niem jmudzik pochuna rata,\n",
            "Cine płach szuzkłukkie,\n",
            "Niaru kał -- chum obogie Wia w chprzydziałody słowałaje obu druch weska i i jak skrotcza wsisze oboparząskeli jepro \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 1399/3000 [1:15:35<1:26:29,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[75m 38s (1400 46%) 2.2737]\n",
            "Wh mala wustał  niege;\n",
            "Zarzedzi ubwa nietrańne żełe drzem wzarwa.\n",
            "Szona.\n",
            "Juchna stadzaka i czajemilachniéj i zliczaladem; gończe w czały rzewiał mrzeńdłe,\n",
            "Kotrene an rawił zruczy w grozala dęgenka? go w \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|████▉     | 1499/3000 [1:21:00<1:21:06,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[81m 4s (1500 50%) 2.4895]\n",
            "Why miąm Praci,\n",
            "Wtuść przysówcinie, ustać proscył jucho,\n",
            "W ulko ztrzybtaść, a nobostrzy włowenam niepodnoci, niu biecigały Paszyć\n",
            "Jam bo pczyskiu, Wosiaś- uszłar oskowieli, rości stolszych,\n",
            "Do się je sk \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 1599/3000 [1:26:25<1:15:43,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[86m 28s (1600 53%) 2.5355]\n",
            "Why klapokny bisku.\n",
            "Płoć sajnicom co się na mosikone.\n",
            "Sposcyła: Wiertaranie zak pałało się bała przysłudzo nien zkoło na:\n",
            "Awią, sto Pomacie,\n",
            "Poczyć na powastkie stać, mie wicę ralę kajeka siesię i ił so \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 1699/3000 [1:31:50<1:10:19,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[91m 54s (1700 56%) 2.3481]\n",
            "Whierwał isał,\n",
            "Odwrożego ona adły i się wowała zawiałe rgrzaty,\n",
            "Oż zaczuchni\n",
            "   dałaganiech wspodadziewana ciożsczy odłoże,\n",
            "Leżestonoła szwieny,\n",
            "Że dwytała, wiiyty\n",
            "Góre ków openegugo waszcześ caniego wa \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 1799/3000 [1:37:15<1:04:55,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[97m 19s (1800 60%) 2.3356]\n",
            "Whiebiona.\n",
            "Ta biewnie niena obrzy buł zakłi do nieżonogy mitki, dynia wywał krył i że,\n",
            "Aż jie jieba wiczywy nikie to oćtał, Żo oroz wyżał, w niem:\n",
            "Jał muż płykni mielnimieni skikim so niemi wysież seby  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 1899/3000 [1:42:41<59:32,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[102m 44s (1900 63%) 2.1848]\n",
            "Why trzed puchkając podzakał kowięż dałe w gtu wech troczana przech nie, cię ubnie, ka dz to królańch manna krała Rawią, wssabiem się werknuch krani.\n",
            "W sto Hrwacha i w to ochrzebié niego kałeby słorała, \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 1999/3000 [1:48:06<54:08,  3.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[108m 10s (2000 66%) 2.3893]\n",
            "Whał i jalo źarzy,\n",
            "A -- Kłodziem udrzykaje zisczyna że gjedam gośny drajec ugałe gaku się zedasna,\n",
            "Naśła s do zpodyna jiena wiedzieże,\n",
            "Przydno ukolaly ogamalic rzokarcusze nażnone osię zagubem się banie \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|██████▉   | 2099/3000 [1:53:32<48:44,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[113m 35s (2100 70%) 2.2123]\n",
            "Whoszie gadny nalejnia pokiedy do rzyszykowiewszył,\n",
            "I dło kłowu, Ty tę zał s się się grzeusów nie sami doję rzewocie kylko powoda go ż do wiedywy się przadźny gojad nich wygo w do połed zagał moska się  \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 2199/3000 [1:58:56<43:19,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[119m 0s (2200 73%) 2.3651]\n",
            "Whnikaje? do krzeka krówi\n",
            " -- przycha nie, kasznie? zysłał?,\n",
            "Nali kowienia do tywle zwat, portest, tach artąch s wzanenia Solane jewiecyny?\n",
            "\n",
            "Tska tel wyszczyłyput podem ukasał że krone,\n",
            "Żyrłi Małył prza \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 2299/3000 [2:04:20<37:54,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[124m 24s (2300 76%) 2.5122]\n",
            "Whru przyczuł te w drzerzy\n",
            "Wiem wskony dzierki sstakiem wiekę sku po robna odwiere w wsoderwie pogy chane w udymowkam uskim,\n",
            "Kodkieniem zażbem kachunki parwiała porów wykrumie starwegam zakrykch dziewsz \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 2399/3000 [2:09:45<32:30,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[129m 49s (2400 80%) 2.4154]\n",
            "Whoły pare spowiskichy.\n",
            "Talę się w Pęną ubię niku\n",
            "Biém czynéj nie miesz i mrwiedziéj tozoco w nie go rzech wieniedzydy krzylkie téj się gasski,\n",
            "Posta suczę wrabie mo nieszcz, s mod w tak drzeborząd w gt \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 2499/3000 [2:15:09<27:05,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[135m 13s (2500 83%) 2.4353]\n",
            "Whałoda powalał namąc, saba! Frzykłowodaniewić włoby wyskanimu, zaszestew gorny; pamiezeby omów przykoławoraniewowę pocze dzańniemniem korza opowina;\n",
            "Zemicio mispawał komicoć czania gdzównyc niejski wie \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 2599/3000 [2:20:34<21:41,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[140m 38s (2600 86%) 2.3461]\n",
            "Wha posto się wywołą pomonie sprzed dudywiem,\n",
            "Międdosi się  s po brad pojdecy na ko wy ka s do w Isielka.\n",
            "Mraszą pieskinanie pościć;\n",
            "Tadeudzę; nie przyce ciena wpusie chwad pajen niewa,\n",
            "W midziecie w ty \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|████████▉ | 2699/3000 [2:25:59<16:16,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[146m 3s (2700 90%) 2.3756]\n",
            "Why; i nyżyć sko sanum,\n",
            "Stowe wiech dohydnem hozów.\n",
            "Mierz mu brówiewsze --\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   \n",
            "Mokone, leszkienie zykrze upliwéj rofięki, strzed powiese. Roszewą, i wdziżeli,\n",
            "Śchył\n",
            "Megłeby.\n",
            "-- Wiedzielko, ptenczéj \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 2799/3000 [2:31:24<10:52,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[151m 28s (2800 93%) 2.2747]\n",
            "Whiem mieł gęch zokieni pomy co niedzyk Hruśmie Pawicie po brasto skórem chwiek: zwumiejszeniek pogrzycze pyte,\n",
            "I wpomny króbki ostrzomskako --\n",
            "I goskiem, bak gokane i cikie, chony jertych w brałko wybr \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 2899/3000 [2:36:50<05:27,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[156m 53s (2900 96%) 2.2148]\n",
            "Wh gędzielganej pugiły\n",
            "Rakulki gród zastaw dego rzedzieciu, gogna litkł się wi ośmieskem rzedzią, i z mielech przupienie dkieś\n",
            "zgęmie dług ostył,\n",
            "A się z małyi się siwidsczy strowsze,\n",
            "Znak się głodo grz \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 2999/3000 [2:42:13<00:03,  3.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[162m 17s (3000 100%) 2.4987]\n",
            "Whwię,\n",
            "I pode pod krzéj,\n",
            "Taje lade potrowie, jak siaz tacu wy ustuwo na się na w sty niegzuczelka proszcza rzagi cieś pogu;\n",
            "Dadzie do a charwy kanie spobią:\n",
            "Gaz spagi! z cz dażéj wyrzy się pojerwać:\n",
            "Sta \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 3000/3000 [2:42:17<00:00,  3.25s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "dXrPD94SXM5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Rysowanie wykresu funkcji straty\n",
        "\n",
        "Wykreślanie historycznej straty z all_losses pokazuje uczenie sieci:"
      ]
    },
    {
      "metadata": {
        "id": "meKCxPo3XM5n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "40d7a20b-8f87-4d33-d6a2-5eb7ea7478dc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795314903,
          "user_tz": -120,
          "elapsed": 1510,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1327d76b70>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyIUQwuYkkAshhM39f+GeUVYhd1AyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f13a869b0f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LJA5RhDWTdp_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71017af0-6701-4031-d395-bac8db8ecd95",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795316844,
          "user_tz": -120,
          "elapsed": 1874,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!uptime"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 16:01:56 up  2:59,  0 users,  load average: 1.00, 1.00, 1.00\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bFJdzy4PTnsO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "867e92af-333e-4a8f-c49d-503b01f92c82",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795317277,
          "user_tz": -120,
          "elapsed": 407,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "who"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path\t RNN\t USE_GPU\t Variable\t all_characters\t all_losses\t char_tensor\t chunk_len\t criterion\t \n",
            "dataset_path\t decoder\t decoder_optimizer\t e\t epoch\t evaluate\t file\t file_len\t fn\t \n",
            "hidden_size\t loss\t loss_avg\t lr\t math\t n_characters\t n_epochs\t n_layers\t nn\t \n",
            "os\t plot_every\t plt\t print_every\t print_memsize\t psutil\t random\t random_chunk\t random_training_set\t \n",
            "re\t start\t string\t ticker\t time\t time_since\t tmp_path\t to_gpu\t torch\t \n",
            "tqdm\t train\t \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2fEFWi-FXM5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ocena w różnych \"temperaturach\"\n",
        "\n",
        "W powyższej funkcji `evaluate`, za każdym razem, gdy dokonywana jest prognoza, wyjścia są dzielone przez przekazany argument \"temperature\". Używanie większej liczby sprawia, że wszystkie akcje są bardziej jednakowo prawdopodobne, a tym samym dają nam \"bardziej losowe\" (\"more random\") wyniki. Używanie mniejszej wartości (mniejszej niż 1) sprawia, że wysokie prawdopodobieństwa przyczyniają się bardziej. Gdy ustawiamy temperaturę na zero, wybieramy tylko najbardziej prawdopodobne wyjścia.\n",
        "\n",
        "Możemy to zobaczyć poprzez dostosowanie argumentu `temperature`.\n"
      ]
    },
    {
      "metadata": {
        "id": "gBm5ibSnXM5p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "791d8e4d-9ef6-4b36-a2e0-9749d53713a0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795318273,
          "user_tz": -120,
          "elapsed": 961,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=0.8))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Th szłune i zuła kradzić, Wapówon,\n",
            "Coby, pażyli po od w sierząża w poczy zabuuczyć na ziatkie bania!\n",
            "\n",
            "Ta zgrzebiwą znałko, z błowoszki munią wiajsze tażwiał cz kofakienie;\n",
            "\n",
            "Tadzy.\n",
            "\n",
            "Tuża szysta grzest st\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DnStt-FzXM5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[link text](https://)Lower temperatures are less varied, choosing only the more probable outputs:"
      ]
    },
    {
      "metadata": {
        "id": "D3mkq_sCXM5r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c574d9a0-173e-4089-b5be-a8c7fbbaf472",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795319265,
          "user_tz": -120,
          "elapsed": 968,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=0.2))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Th się się się pod usta się szarzy spanie pawie się się stawie w poszycz pod poszczyn w w poszyć się pod poszanie w się się w zabawnie w się się pod się w do pod się w się się się pod wielnie,\n",
            "I się z s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l508QP1LXM5t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Higher temperatures more varied, choosing less probable outputs:"
      ]
    },
    {
      "metadata": {
        "id": "SxhfgI-PXM5u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4c55b602-a5ac-43cc-f4b9-3271296c4356",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795320268,
          "user_tz": -120,
          "elapsed": 978,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(evaluate('Th', 200, temperature=1.4))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thażew glują\n",
            "Obająge wepy gropa nachwieg w oć oka- roy\n",
            "trzątam Abia.\n",
            "Néj na w nieźszcza\n",
            "Bostaśli,\n",
            "Nugą ę s:-- uciarwadzi tacóna bogu!\n",
            "Nnam larbąryny możodanać spłostąć\n",
            "Téczniéj mowonąć;\n",
            "Agazaz pokał mow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-UQoZYk2XM5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ćwiczenia\n",
        "\n",
        "* Trenuj z własnym zestawem danych, np.\n",
        "     * Tekst od innego autora\n",
        "     * Posty na blogu\n",
        "     * Kod\n",
        "* Zwiększ liczbę warstw i rozmiar sieci, aby uzyskać lepsze wyniki"
      ]
    },
    {
      "metadata": {
        "id": "pFHHvkI6XM5w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Następnie**: [Generowanie nazw z warunkowym poziomem RNN na poziomie znaku](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "EtK-9BuuB-GX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Zapisywanie modelu"
      ]
    },
    {
      "metadata": {
        "id": "aSA3Utpc6dXV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ALLCHARS, MODEL = ['all_characters', 'model']\n",
        "\n",
        "fn_pan_tadeusz = {ALLCHARS: 'all_characters.pan_tadeusz.p', MODEL: 'pan_tadeusz.h400.l3.3000.gpu.torch'}\n",
        "\n",
        "fn_dict = fn_pan_tadeusz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ-j5_4I6d0g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save all_characters\n",
        "import pickle\n",
        "\n",
        "pickle.dump(all_characters, open(tmp_path / fn_dict[ALLCHARS], 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDm3bjJy6d3S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# all_characters = pickle.load( open( tmp_path / fn_dict[ALLCHARS], 'rb' ) )\n",
        "# n_characters = len(all_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IwOSEhKulCwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qHbGgVULB_cp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4297fe1b-0f18-40d0-f7d6-ba25043080b6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795323213,
          "user_tz": -120,
          "elapsed": 691,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "decoder.state_dict"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of RNN(\n",
              "  (encoder): Embedding(89, 300)\n",
              "  (gru): GRU(300, 300, num_layers=4)\n",
              "  (decoder): Linear(in_features=300, out_features=89)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "nRXLplmTCumA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "14d3aa5f-1736-479c-f32e-720a1594dee2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795324005,
          "user_tz": -120,
          "elapsed": 747,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model_path = tmp_path / fn_dict[MODEL]\n",
        "torch.save(decoder, model_path)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-D9_ONXNMHPK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        " # decoder = torch.load(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tm9jbTN6Wfl-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d4d5a41-7acf-4825-d596-23e357d598ae",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795325642,
          "user_tz": -120,
          "elapsed": 819,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_path"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data/rnn_generator/tmp/pan_tadeusz.h400.l3.3000.gpu.torch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "Dt8_7GshWi4M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8b0b1926-ca6b-499c-89ca-400c249fb53b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525795327519,
          "user_tz": -120,
          "elapsed": 1839,
          "user": {
            "displayName": "Kamsiulek Malutki",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118138569128465864586"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls -lah /content/data/rnn_generator/tmp/"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8.5M\r\n",
            "drwxr-xr-x 2 root root 4.0K May  8 16:02 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
            "drwxr-xr-x 3 root root 4.0K May  8 13:19 \u001b[01;34m..\u001b[0m/\r\n",
            "-rw-r--r-- 1 root root  743 May  8 16:02 all_characters.pan_tadeusz.p\r\n",
            "-rw-r--r-- 1 root root 8.5M May  8 16:02 pan_tadeusz.h400.l3.3000.gpu.torch\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}